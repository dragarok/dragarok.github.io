<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Alok&#39;s Blog</title>
    <link>/</link>
    <description>Recent content on Alok&#39;s Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 14 Apr 2020 07:53:22 +0545</lastBuildDate>
    
	<atom:link href="/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Algorithms</title>
      <link>/braindump/2020/04/algorithms/</link>
      <pubDate>Tue, 21 Apr 2020 06:02:46 +0545</pubDate>
      
      <guid>/braindump/2020/04/algorithms/</guid>
      <description>Algorithms Basic rules of complexity definition for removing terms:
 Multiplicative constants can be omitted: 14n2 becomes n2. n^a dominates n^b if a &amp;gt; b: for instance, n^2 dominates n. Any exponential dominates any polynomial: 3^n dominates n^5 (it even dominates 2^n). Likewise, any polynomial dominates any logarithm: n dominates (log n)^3. This also means, for example, that n^2 dominates n* log n.  Techniques to think of counter examples:</description>
    </item>
    
    <item>
      <title>Graph Signal Processing</title>
      <link>/braindump/2020/04/graph-signal-processing/</link>
      <pubDate>Tue, 21 Apr 2020 05:57:03 +0545</pubDate>
      
      <guid>/braindump/2020/04/graph-signal-processing/</guid>
      <description>&amp;ndash; tags: Graph Convolutions
Basics Spectral Graph Convolutional Network:  GSP is the key to generalizing convolutions, allowing us to build functions that can take into account both the overall structure of the graph and the individual properties of the graph’s components. It is the graph fourier transform that allows one to introduce the notion of a “bandwidth” or “smoothness” to a graph In GCNs, node features and attributes are represented by “signals”.</description>
    </item>
    
    <item>
      <title>Graph Convolutions</title>
      <link>/braindump/2020/04/graph-convolutions/</link>
      <pubDate>Tue, 21 Apr 2020 05:54:09 +0545</pubDate>
      
      <guid>/braindump/2020/04/graph-convolutions/</guid>
      <description>Graph Conv &amp;ndash; What is inductive bias ? &amp;ndash; Inductive bias of a learning algorithm is the set of assumptions that the learner uses to predict outputs given inputs that it has not encountered. &amp;ndash; Neural Networks tend to discard the features that have very less significance in task at hand. &amp;ndash; Similary, GNNs have bias: By structuring the data in a way that prioritizes certain patterns, we can improve model performance, even if the data is the same.</description>
    </item>
    
    <item>
      <title>Fast GraphConv Nets</title>
      <link>/braindump/2020/04/fast-graphconv-nets/</link>
      <pubDate>Tue, 21 Apr 2020 05:53:45 +0545</pubDate>
      
      <guid>/braindump/2020/04/fast-graphconv-nets/</guid>
      <description>&amp;ndash; tags: Graph Signal Processing
Intro &amp;ndash; The kernel is defined in Fourier space and graph Fourier transforms are notoriously expensive to compute. &amp;ndash; O(N²) operation for a graph with N nodes: the multiplication operation for i/p nodes and eigenvec of graph&amp;rsquo;s laplacian. &amp;ndash; Thus as a solution to the computational complexity introduced by it, Monte Carlo (biased random sampling method) approaches to consistently estimate the integrals, which allowed for batch training, reducing the overall training time.</description>
    </item>
    
    <item>
      <title>MoNets</title>
      <link>/braindump/2020/04/monets/</link>
      <pubDate>Tue, 21 Apr 2020 05:53:35 +0545</pubDate>
      
      <guid>/braindump/2020/04/monets/</guid>
      <description>&amp;ndash; tags: Graph Signal Processing
MoNets: &amp;ndash; Base of research such as Spline CNNs, Geodesic, Anisotropic CNNs etc. &amp;ndash; Contributions: &amp;ndash; A generalization of various Graph Learning approaches, unifying spatial and spectral approaches &amp;ndash; A new approach using parametric kernels, pseudo-coordinates, integrated with existing models (Anistropic CNN, Geodesic CNN,) &amp;ndash; A series of experiments performed on different benchmark manifolds, graphs, and networks &amp;ndash; MoNet’s generalization first considers variable x as a point in the manifold or node in a graph depending on the application, task, and input.</description>
    </item>
    
    <item>
      <title>Motifs</title>
      <link>/braindump/2020/04/motifs/</link>
      <pubDate>Tue, 21 Apr 2020 05:53:26 +0545</pubDate>
      
      <guid>/braindump/2020/04/motifs/</guid>
      <description>&amp;ndash; tags: Graph Signal Processing
Intro &amp;ndash; Essentially, the mode partitions input graphs into motifs which are unique substructures of x length, such that any graph in the dataset can be built by some combination of the motifs. &amp;ndash; The motifs take into directionality of edges into consideration, a detail in graph theory that has been omitted in previous graph learning approaches. &amp;ndash; Each convolutional layer of a MotifNet has a multivariate matrix polynomial (a fancy kernel where each element is a polynomial with multiple variables), which is applied to and learns from the motif’s Laplacian matrices.</description>
    </item>
    
    <item>
      <title>ChebNets</title>
      <link>/braindump/2020/04/chebnets/</link>
      <pubDate>Tue, 21 Apr 2020 05:53:09 +0545</pubDate>
      
      <guid>/braindump/2020/04/chebnets/</guid>
      <description>&amp;ndash; tags: Graph Signal Processing
ChebNets &amp;ndash; Spectral convolutions are defined as the multiplication of a signal (node features/attributes) by a kernel. &amp;ndash; Thus similar to original convolution operation &amp;ndash; The kernel used in a spectral convolution made of Chebyshev polynomials of the diagonal matrix of Laplacian eigenvalues &amp;ndash; The kernel equals the sum of all Chebyshev polynomial kernels applied to the diagonal matrix of scaled Laplacian eigenvalues for each order of k up to K-1.</description>
    </item>
    
    <item>
      <title>Cayley Nets</title>
      <link>/braindump/2020/04/cayley-nets/</link>
      <pubDate>Tue, 21 Apr 2020 05:52:54 +0545</pubDate>
      
      <guid>/braindump/2020/04/cayley-nets/</guid>
      <description>&amp;ndash; tags: Graph Signal Processing
Intro &amp;ndash; Cayley Transform: Unit half circle transform &amp;ndash; useful notion of localization &amp;ndash; Cayley has proven to perform better on a wide range of Graph Learning tasks due to their ability to detect narrow frequency bands of importance during training, and to specialize on them while being well-localized on the graph. </description>
    </item>
    
    <item>
      <title>Graph Data Structures</title>
      <link>/braindump/2020/04/graph-data-structures/</link>
      <pubDate>Tue, 21 Apr 2020 05:52:14 +0545</pubDate>
      
      <guid>/braindump/2020/04/graph-data-structures/</guid>
      <description>&amp;ndash;tags: Data Structures
Graphs:  Siblings and parent nodes have restriction in trees so graph removes it No starting node or root node Transport or Network points can be explained using graphs Vertices and edges: Sets of vertices and edges Directed or undirected graph ; also weighted graph to give weight to the edges ; also can be used for priority Linked list, Tress , Heaps are specific graphs with some sorts of constraints  </description>
    </item>
    
    <item>
      <title>Array Data Structures</title>
      <link>/braindump/2020/04/array-data-structures/</link>
      <pubDate>Tue, 21 Apr 2020 05:51:27 +0545</pubDate>
      
      <guid>/braindump/2020/04/array-data-structures/</guid>
      <description>&amp;ndash;tags: Data Structures
Arrays:  Ordered collection of items with an index Index used to find the data Zero based index Fixed size - they are immutable dynamically add or remove elements in runtime data type is mostly specific the more constraints you put into data type ; the smaller efficient and practical use Flexibility comes at the cost of performance and memory  Multidimensional arrays:  Rows and columns: Array of arrays different dimensional arrays: collection of n-1 arrays is the n- dimensional arrays Multiple locations: Sets of days and time for the weather is a 3d array  Jagged arrays:  Not only rectangular arrays For days in months: there are relevant places where data is empty Internal arrays having different lengths new int[12][] gives us feasibility to write different one dimensional arrays in different situations  Resizable arrays:  all arrays that are defined firstly are immutable array list is different and instantiated as an object Objectives C gives the NSArray is immutable  NSMutableArray has mutability    Question:  Adding element at the top or the bottom Add the element at specific position Larger the array there is performance lag   Same functionality is used in a different way in every programming language pop is used mostly to remove the item in a array  Sorting in an array:  Internal structure and index remains the same Colossal amount of data if there are thousands of data to work with Different sorting algorithms on the basis of data size So, to choose different types and different mechanisms for sorting on the basis of how frequently the data needs to be sorted and how much data is to be sorted  Arrays of custom objects SORT:  Information to sort on which order Look into these objects and sort according to some data inside the object Bit of logic: making a comparator function to order or sort the objects  Searching arrays:  Sequential search to find the element Linear time algorithm Need to check every data just to find the data and predictable sequence to find data easily Cannot have a data structure that is equally good in all situations  Built in search behavior:  .</description>
    </item>
    
  </channel>
</rss>