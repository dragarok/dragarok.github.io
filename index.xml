<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Alok&#39;s Blog</title>
    <link>/</link>
    <description>Recent content on Alok&#39;s Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 14 Apr 2020 07:53:22 +0545</lastBuildDate>
    
	<atom:link href="/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>RMS recon</title>
      <link>/braindump/2020/04/rms-recon/</link>
      <pubDate>Tue, 21 Apr 2020 07:08:13 +0545</pubDate>
      
      <guid>/braindump/2020/04/rms-recon/</guid>
      <description>TRAN TRAN_A Index: 1,1,1,
  PACK_IND
Indicates whether or not the item is a pack item.
    TRAN_CODE
Tran_code : Identifies the transaction type. Valid values are: 0 Ordering 1 Net Sales 2 Net Sales VAT Exclusive 3 Non-inventory Items Sales/Returns 4 Returns 5 Non-inventory VAT Exclusive Sales 6 Deals income (Sales) 7 Deals income (Purchases) 8 Fixed Income Accrual 10
    UNITS</description>
    </item>
    
    <item>
      <title>Algorithms</title>
      <link>/braindump/2020/04/algorithms/</link>
      <pubDate>Tue, 21 Apr 2020 06:02:46 +0545</pubDate>
      
      <guid>/braindump/2020/04/algorithms/</guid>
      <description>Algorithms Basic rules of complexity definition for removing terms:
 Multiplicative constants can be omitted: 14n2 becomes n2. n^a dominates n^b if a &amp;gt; b: for instance, n^2 dominates n. Any exponential dominates any polynomial: 3^n dominates n^5 (it even dominates 2^n). Likewise, any polynomial dominates any logarithm: n dominates (log n)^3. This also means, for example, that n^2 dominates n* log n.  Techniques to think of counter examples:</description>
    </item>
    
    <item>
      <title>Graph Signal Processing</title>
      <link>/braindump/2020/04/graph-signal-processing/</link>
      <pubDate>Tue, 21 Apr 2020 05:57:03 +0545</pubDate>
      
      <guid>/braindump/2020/04/graph-signal-processing/</guid>
      <description>&amp;ndash; tags: Graph Convolutions
Basics Spectral Graph Convolutional Network:  GSP is the key to generalizing convolutions, allowing us to build functions that can take into account both the overall structure of the graph and the individual properties of the graph’s components. It is the graph fourier transform that allows one to introduce the notion of a “bandwidth” or “smoothness” to a graph In GCNs, node features and attributes are represented by “signals”.</description>
    </item>
    
    <item>
      <title>Graph Convolutions</title>
      <link>/braindump/2020/04/graph-convolutions/</link>
      <pubDate>Tue, 21 Apr 2020 05:54:09 +0545</pubDate>
      
      <guid>/braindump/2020/04/graph-convolutions/</guid>
      <description>Graph Conv &amp;ndash; What is inductive bias ? &amp;ndash; Inductive bias of a learning algorithm is the set of assumptions that the learner uses to predict outputs given inputs that it has not encountered. &amp;ndash; Neural Networks tend to discard the features that have very less significance in task at hand. &amp;ndash; Similary, GNNs have bias: By structuring the data in a way that prioritizes certain patterns, we can improve model performance, even if the data is the same.</description>
    </item>
    
    <item>
      <title>Fast GraphConv Nets</title>
      <link>/braindump/2020/04/fast-graphconv-nets/</link>
      <pubDate>Tue, 21 Apr 2020 05:53:45 +0545</pubDate>
      
      <guid>/braindump/2020/04/fast-graphconv-nets/</guid>
      <description>&amp;ndash; tags: Graph Signal Processing
Intro &amp;ndash; The kernel is defined in Fourier space and graph Fourier transforms are notoriously expensive to compute. &amp;ndash; O(N²) operation for a graph with N nodes: the multiplication operation for i/p nodes and eigenvec of graph&amp;rsquo;s laplacian. &amp;ndash; Thus as a solution to the computational complexity introduced by it, Monte Carlo (biased random sampling method) approaches to consistently estimate the integrals, which allowed for batch training, reducing the overall training time.</description>
    </item>
    
    <item>
      <title>MoNets</title>
      <link>/braindump/2020/04/monets/</link>
      <pubDate>Tue, 21 Apr 2020 05:53:35 +0545</pubDate>
      
      <guid>/braindump/2020/04/monets/</guid>
      <description>&amp;ndash; tags: Graph Signal Processing
MoNets: &amp;ndash; Base of research such as Spline CNNs, Geodesic, Anisotropic CNNs etc. &amp;ndash; Contributions: &amp;ndash; A generalization of various Graph Learning approaches, unifying spatial and spectral approaches &amp;ndash; A new approach using parametric kernels, pseudo-coordinates, integrated with existing models (Anistropic CNN, Geodesic CNN,) &amp;ndash; A series of experiments performed on different benchmark manifolds, graphs, and networks &amp;ndash; MoNet’s generalization first considers variable x as a point in the manifold or node in a graph depending on the application, task, and input.</description>
    </item>
    
    <item>
      <title>Motifs</title>
      <link>/braindump/2020/04/motifs/</link>
      <pubDate>Tue, 21 Apr 2020 05:53:26 +0545</pubDate>
      
      <guid>/braindump/2020/04/motifs/</guid>
      <description>&amp;ndash; tags: Graph Signal Processing
Intro &amp;ndash; Essentially, the mode partitions input graphs into motifs which are unique substructures of x length, such that any graph in the dataset can be built by some combination of the motifs. &amp;ndash; The motifs take into directionality of edges into consideration, a detail in graph theory that has been omitted in previous graph learning approaches. &amp;ndash; Each convolutional layer of a MotifNet has a multivariate matrix polynomial (a fancy kernel where each element is a polynomial with multiple variables), which is applied to and learns from the motif’s Laplacian matrices.</description>
    </item>
    
    <item>
      <title>ChebNets</title>
      <link>/braindump/2020/04/chebnets/</link>
      <pubDate>Tue, 21 Apr 2020 05:53:09 +0545</pubDate>
      
      <guid>/braindump/2020/04/chebnets/</guid>
      <description>&amp;ndash; tags: Graph Signal Processing
ChebNets &amp;ndash; Spectral convolutions are defined as the multiplication of a signal (node features/attributes) by a kernel. &amp;ndash; Thus similar to original convolution operation &amp;ndash; The kernel used in a spectral convolution made of Chebyshev polynomials of the diagonal matrix of Laplacian eigenvalues &amp;ndash; The kernel equals the sum of all Chebyshev polynomial kernels applied to the diagonal matrix of scaled Laplacian eigenvalues for each order of k up to K-1.</description>
    </item>
    
    <item>
      <title>Cayley Nets</title>
      <link>/braindump/2020/04/cayley-nets/</link>
      <pubDate>Tue, 21 Apr 2020 05:52:54 +0545</pubDate>
      
      <guid>/braindump/2020/04/cayley-nets/</guid>
      <description>&amp;ndash; tags: Graph Signal Processing
Intro &amp;ndash; Cayley Transform: Unit half circle transform &amp;ndash; useful notion of localization &amp;ndash; Cayley has proven to perform better on a wide range of Graph Learning tasks due to their ability to detect narrow frequency bands of importance during training, and to specialize on them while being well-localized on the graph. </description>
    </item>
    
    <item>
      <title>Graph Data Structures</title>
      <link>/braindump/2020/04/graph-data-structures/</link>
      <pubDate>Tue, 21 Apr 2020 05:52:14 +0545</pubDate>
      
      <guid>/braindump/2020/04/graph-data-structures/</guid>
      <description>&amp;ndash;tags: Data Structures
Graphs:  Siblings and parent nodes have restriction in trees so graph removes it No starting node or root node Transport or Network points can be explained using graphs Vertices and edges: Sets of vertices and edges Directed or undirected graph ; also weighted graph to give weight to the edges ; also can be used for priority Linked list, Tress , Heaps are specific graphs with some sorts of constraints  </description>
    </item>
    
  </channel>
</rss>