<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="ie=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="author" content="[Alok Regmi]">
<meta name="description" content="&ndash; tags: Graph Signal Processing
MoNets: &ndash; Base of research such as Spline CNNs, Geodesic, Anisotropic CNNs etc. &ndash; Contributions: &ndash; A generalization of various Graph Learning approaches, unifying spatial and spectral approaches &ndash; A new approach using parametric kernels, pseudo-coordinates, integrated with existing models (Anistropic CNN, Geodesic CNN,) &ndash; A series of experiments performed on different benchmark manifolds, graphs, and networks &ndash; MoNet’s generalization first considers variable x as a point in the manifold or node in a graph depending on the application, task, and input. Variables y is considered the neighboring nodes or points, which is associated with a vector of pseudo-coordinates in d-dimensional space, such that u(x, y) are the pseudo-coordinates of which there is a unique set for each neighbor of x. &ndash; Each pseudo-coordinate is put through a weighting function, which replicates the effect of a traditional image convolution kernel whereby each value of the kernel is multiplied by the value that is currently in consideration. In the case of MoNet, the weighting function is a kernel with learnable parameters that operates on the pseudo-coordinates:
" />
<meta name="keywords" content="Blog" />
<meta name="robots" content="noodp" />
<meta name="theme-color" content="" />
<link rel="canonical" href="https://dragarok.github.io/ai/monets/" />


    <title>
        
            MoNets ::  
        
    </title>



<link href="https://cdnjs.cloudflare.com/ajax/libs/flag-icon-css/3.2.1/css/flag-icon.min.css" rel="stylesheet"
    type="text/css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">
    
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js" integrity="sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4" crossorigin="anonymous"></script>
    
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js" integrity="sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa" crossorigin="anonymous"
    onload="renderMathInElement(document.body);"></script>





<link rel="stylesheet" href="../../main.min.b0d234ca364f76d18f74dd789a642d763e058fe48a746787387c0cdd21815e04.css">




    <link rel="apple-touch-icon" sizes="180x180" href="../../apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="../../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../../favicon-16x16.png">
    <link rel="manifest" href="../../site.webmanifest">
    <link rel="mask-icon" href="../../safari-pinned-tab.svg" color="#252627">
    <link rel="shortcut icon" href="../../favicon.ico">
    <meta name="msapplication-TileColor" content="#252627">
    <meta name="theme-color" content="#252627">


  <meta itemprop="name" content="MoNets">
  <meta itemprop="description" content="– tags: Graph Signal Processing
MoNets: – Base of research such as Spline CNNs, Geodesic, Anisotropic CNNs etc. – Contributions: – A generalization of various Graph Learning approaches, unifying spatial and spectral approaches – A new approach using parametric kernels, pseudo-coordinates, integrated with existing models (Anistropic CNN, Geodesic CNN,) – A series of experiments performed on different benchmark manifolds, graphs, and networks – MoNet’s generalization first considers variable x as a point in the manifold or node in a graph depending on the application, task, and input. Variables y is considered the neighboring nodes or points, which is associated with a vector of pseudo-coordinates in d-dimensional space, such that u(x, y) are the pseudo-coordinates of which there is a unique set for each neighbor of x. – Each pseudo-coordinate is put through a weighting function, which replicates the effect of a traditional image convolution kernel whereby each value of the kernel is multiplied by the value that is currently in consideration. In the case of MoNet, the weighting function is a kernel with learnable parameters that operates on the pseudo-coordinates:">
  <meta itemprop="datePublished" content="2020-03-12T00:00:00+00:00">
  <meta itemprop="dateModified" content="2020-05-02T03:43:31+05:45">
  <meta itemprop="wordCount" content="334">
  <meta itemprop="image" content="https://dragarok.github.io/">
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:image" content="https://dragarok.github.io/">
  <meta name="twitter:title" content="MoNets">
  <meta name="twitter:description" content="– tags: Graph Signal Processing
MoNets: – Base of research such as Spline CNNs, Geodesic, Anisotropic CNNs etc. – Contributions: – A generalization of various Graph Learning approaches, unifying spatial and spectral approaches – A new approach using parametric kernels, pseudo-coordinates, integrated with existing models (Anistropic CNN, Geodesic CNN,) – A series of experiments performed on different benchmark manifolds, graphs, and networks – MoNet’s generalization first considers variable x as a point in the manifold or node in a graph depending on the application, task, and input. Variables y is considered the neighboring nodes or points, which is associated with a vector of pseudo-coordinates in d-dimensional space, such that u(x, y) are the pseudo-coordinates of which there is a unique set for each neighbor of x. – Each pseudo-coordinate is put through a weighting function, which replicates the effect of a traditional image convolution kernel whereby each value of the kernel is multiplied by the value that is currently in consideration. In the case of MoNet, the weighting function is a kernel with learnable parameters that operates on the pseudo-coordinates:">





    <meta property="article:published_time" content="2020-03-12 00:00:00 &#43;0000 UTC" />








    </head>

    <body class="">
        <div class="container">
            <header class="header">
    <span class="header__inner">
        <a href="../../" style="text-decoration: none;">
    <div class="logo">
        
            <span class="logo__mark">~</span>
            <span class="logo__text">K4iv41y4</span>
            <span class="logo__mark">&nbsp;~</span>
            <span class="logo__cursor" style=
                  "
                   
                   ">
            </span>
        
    </div>
</a>


        <span class="header__right">
            
                <nav class="menu">
    <ul class="menu__inner"><li><a href="https://dragarok.github.io/about/">About</a></li><li><a href="https://dragarok.github.io/ai/">AI</a></li><li><a href="https://dragarok.github.io/posts/">Blog</a></li><li><a href="https://dragarok.github.io/books/">Books</a></li><li><a href="https://dragarok.github.io/braindump/">Braindump</a></li>
    </ul>
</nav>

                <span class="menu-trigger">
                    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
                        <path d="M0 0h24v24H0z" fill="none"/>
                        <path d="M3 18h18v-2H3v2zm0-5h18v-2H3v2zm0-7v2h18V6H3z"/>
                    </svg>
                </span>
            

            <span class="theme-toggle unselectable"><svg class="theme-toggler" width="24" height="24" viewBox="0 0 48 48" fill="none" xmlns="http://www.w3.org/2000/svg">
  <path d="M22 41C32.4934 41 41 32.4934 41 22C41 11.5066 32.4934 3 22
  3C11.5066 3 3 11.5066 3 22C3 32.4934 11.5066 41 22 41ZM7 22C7
  13.7157 13.7157 7 22 7V37C13.7157 37 7 30.2843 7 22Z"/>
</svg>
</span>
        </span>
    </span>
</header>


            <div class="content">
                
    <main class="post">

        <div class="post-info">
            
            </p>
        </div>

        <article>
            <h2 class="post-title"><a href="https://dragarok.github.io/ai/monets/">MoNets</a></h2>

            

            <div class="post-content">
                <p>&ndash; tags: <a href="../../ai/graph_signal_processing/">Graph Signal Processing</a></p>
<h2 id="monets">MoNets:</h2>
<p>&ndash; Base of research such as Spline CNNs, Geodesic, Anisotropic CNNs etc.
&ndash; Contributions:
&ndash; A generalization of various Graph Learning approaches, unifying spatial and spectral approaches
&ndash; A new approach using parametric kernels, pseudo-coordinates, integrated with existing models (Anistropic CNN, Geodesic CNN,)
&ndash; A series of experiments performed on different benchmark manifolds, graphs, and networks
&ndash; MoNet’s generalization first considers variable x as a point in the manifold or node in a graph depending on the application, task, and input.
Variables y is considered the neighboring nodes or points, which is associated with a vector of pseudo-coordinates in d-dimensional space,
such that u(x, y) are the pseudo-coordinates of which there is a unique set for each neighbor of x.
&ndash; Each pseudo-coordinate is put through a weighting function, which replicates the effect of a traditional image convolution kernel
whereby each value of the kernel is multiplied by the value that is currently in consideration.
In the case of MoNet, the weighting function is a kernel with learnable parameters that operates on the pseudo-coordinates:</p>
<p>&ndash; we will have covariance matrix of order &rsquo;d&rsquo; by &rsquo;d&rsquo; and mean vector of order 1 by &rsquo;d&rsquo; for the kernel.
&ndash; patch operator weighting functions (fancy way of saying kernel) used in different convolutional operations on the manifold. ( the heatmap figure of 3d models)
&ndash; This kernel is learned only in MoNet, versus hard-coded in GCNNs and ACNNs. The first thing one might notice is how much “smoother” the weights (red curves) are in MoNet,
versus the more distinct (GCNN) and more integrated (ACNN) weights of the other approaches.
&ndash; This flexibility is part of the reason why both allow for the sharing of parameters cross-dataset.
This opens up the capacity for ideas like Transfer Learning to be applied to the geometric domain.</p>
<p>&ndash; Spectral convolutions are mathematically as well as theoretically similar to their vanilla convolution counterparts whereas
spatial convolutions are conceptually as well as markedly similar to vanilla convolutions.</p>

            </div>
        </article>

        <hr />

        <div class="post-info">
  			</div>

        
    </main>

            </div>

        </div>

        




<script type="text/javascript" src="../../bundle.min.74fe699bb673e8362137b575513abfcf73303855d923eea09c0e507deab0ca7f8321880b672790b9e63cc109a18189deebfd13899a8ff536e858791973ffd487.js" integrity="sha512-dP5pm7Zz6DYhN7V1UTq/z3MwOFXZI&#43;6gnA5Qfeqwyn&#43;DIYgLZyeQueY8wQmhgYne6/0TiZqP9TboWHkZc//Uhw=="></script>



    </body>
</html>
