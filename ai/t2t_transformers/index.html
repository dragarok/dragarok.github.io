<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="ie=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="author" content="[Alok Regmi]">
<meta name="description" content="Catches from text to text transfer transformers:
As an example, consider the case of English to German translation: If we have a training datapoint with input sentence “That is good.” and target “Das ist gut.”, we would simply train the model on next-step prediction over the concatenated input sequence “translate English to German: That is good. target: Das ist gut.” If we wanted to obtain the model’s prediction for this example, the model would be fed the prefix “translate English to German: That is good. target:” and would be asked to generate the remainder of the sequence autoregressively.
" />
<meta name="keywords" content="Blog, NLP, neuralnetwork" />
<meta name="robots" content="noodp" />
<meta name="theme-color" content="" />
<link rel="canonical" href="https://dragarok.github.io/ai/t2t_transformers/" />


    <title>
        
            Text-Text Transformers ::  
        
    </title>



<link href="https://cdnjs.cloudflare.com/ajax/libs/flag-icon-css/3.2.1/css/flag-icon.min.css" rel="stylesheet"
    type="text/css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">
    
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js" integrity="sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4" crossorigin="anonymous"></script>
    
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js" integrity="sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa" crossorigin="anonymous"
    onload="renderMathInElement(document.body);"></script>





<link rel="stylesheet" href="../../main.min.3776bc7b2f4134a9ee4c9ed3c7e665d484ac2532e46467d1fa8e55cff5f1566c.css">




    <link rel="apple-touch-icon" sizes="180x180" href="../../apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="../../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../../favicon-16x16.png">
    <link rel="manifest" href="../../site.webmanifest">
    <link rel="mask-icon" href="../../safari-pinned-tab.svg" color="#252627">
    <link rel="shortcut icon" href="../../favicon.ico">
    <meta name="msapplication-TileColor" content="#252627">
    <meta name="theme-color" content="#252627">


  <meta itemprop="name" content="Text-Text Transformers">
  <meta itemprop="description" content="Catches from text to text transfer transformers:
As an example, consider the case of English to German translation: If we have a training datapoint with input sentence “That is good.” and target “Das ist gut.”, we would simply train the model on next-step prediction over the concatenated input sequence “translate English to German: That is good. target: Das ist gut.” If we wanted to obtain the model’s prediction for this example, the model would be fed the prefix “translate English to German: That is good. target:” and would be asked to generate the remainder of the sequence autoregressively.">
  <meta itemprop="datePublished" content="2020-04-03T00:00:00+00:00">
  <meta itemprop="dateModified" content="2020-05-02T03:46:24+05:45">
  <meta itemprop="wordCount" content="328">
  <meta itemprop="image" content="https://dragarok.github.io/">
  <meta itemprop="keywords" content="NLP,Neuralnetwork">
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:image" content="https://dragarok.github.io/">
  <meta name="twitter:title" content="Text-Text Transformers">
  <meta name="twitter:description" content="Catches from text to text transfer transformers:
As an example, consider the case of English to German translation: If we have a training datapoint with input sentence “That is good.” and target “Das ist gut.”, we would simply train the model on next-step prediction over the concatenated input sequence “translate English to German: That is good. target: Das ist gut.” If we wanted to obtain the model’s prediction for this example, the model would be fed the prefix “translate English to German: That is good. target:” and would be asked to generate the remainder of the sequence autoregressively.">





    <meta property="article:published_time" content="2020-04-03 00:00:00 &#43;0000 UTC" />








    </head>

    <body class="">
        <div class="container">
            <header class="header">
    <span class="header__inner">
        <a href="../../" style="text-decoration: none;">
    <div class="logo">
        
            <span class="logo__mark">~</span>
            <span class="logo__text">K4iv41y4</span>
            <span class="logo__mark">&nbsp;~</span>
            <span class="logo__cursor" style=
                  "
                   
                   ">
            </span>
        
    </div>
</a>


        <span class="header__right">
            
                <nav class="menu">
    <ul class="menu__inner"><li><a href="https://dragarok.github.io/about/">About</a></li><li><a href="https://dragarok.github.io/ai/">AI</a></li><li><a href="https://dragarok.github.io/posts/">Blog</a></li><li><a href="https://dragarok.github.io/books/">Books</a></li><li><a href="https://dragarok.github.io/braindump/">Braindump</a></li>
    </ul>
</nav>

                <span class="menu-trigger">
                    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
                        <path d="M0 0h24v24H0z" fill="none"/>
                        <path d="M3 18h18v-2H3v2zm0-5h18v-2H3v2zm0-7v2h18V6H3z"/>
                    </svg>
                </span>
            

            <span class="theme-toggle unselectable"><svg class="theme-toggler" width="24" height="24" viewBox="0 0 48 48" fill="none" xmlns="http://www.w3.org/2000/svg">
  <path d="M22 41C32.4934 41 41 32.4934 41 22C41 11.5066 32.4934 3 22
  3C11.5066 3 3 11.5066 3 22C3 32.4934 11.5066 41 22 41ZM7 22C7
  13.7157 13.7157 7 22 7V37C13.7157 37 7 30.2843 7 22Z"/>
</svg>
</span>
        </span>
    </span>
</header>


            <div class="content">
                
    <main class="post">

        <div class="post-info">
            
            </p>
        </div>

        <article>
            <h2 class="post-title"><a href="https://dragarok.github.io/ai/t2t_transformers/">Text-Text Transformers</a></h2>

            

            <div class="post-content">
                <p>Catches from text to text transfer transformers:</p>
<ol>
<li>As an example, consider the case of English to German translation: If we have a training</li>
</ol>
<p>datapoint with input sentence “That is good.” and target “Das ist gut.”, we would simply train the
model on next-step prediction over the concatenated input sequence “translate English to German:
That is good. target: Das ist gut.” If we wanted to obtain the model’s prediction for this example,
the model would be fed the prefix “translate English to German: That is good. target:” and would
be asked to generate the remainder of the sequence autoregressively.</p>
<ol>
<li>In order to conquer the disadvantage of causal transformers, the prefix LM model tries to</li>
</ol>
<p>avoid this by making the model to see the sequence non causally during the prefix .
e.g in translating from english to german , it sees whole english sequence but causality is
maintained on generated translation.</p>
<ol>
<li>Lesser the parameters lesser is the training loss: this is explained as the model remembering the</li>
</ol>
<p>training examples rather than really learning to do the task.</p>
<ol>
<li>Repeating pre training data is not always harmful but after a certain limit it is. In case of example of</li>
</ol>
<p>C4 database with 2^35 tokens, when using dataset with 2^29 tokens, we can repeat this by 2^6 times means 64. Till
that much the performance doesn&rsquo;t deteriorate.</p>
<ol>
<li>
<p>artificial “limit”</p>
</li>
<li>
<p>Temperature specific mixing</p>
</li>
<li>
<p>We find that fine-tuning after multi-task</p>
</li>
</ol>
<p>pre-training results in comparable performance to our baseline.</p>
<ol>
<li>Interestingly, the performance of “leave-one-out” training was only slightly</li>
</ol>
<p>worse, suggesting that a model that was trained on a variety of tasks can still adapt to new tasks (i.e.
multi-task pre-training might not result in a dramatic task interference)</p>
<ol>
<li>Instead of pre-training 4 separate models, a cheaper alternative is to take</li>
</ol>
<p>a single pre-trained model and produce 4 separate fine-tuned versions</p>
<ol>
<li>Separately, we note that ensembling N separate models</li>
</ol>
<p>has a similar cost to using a model that has an N× higher computational cost.</p>
<ol>
<li>
<p>Language agnostic models</p>
</li>
<li></li>
</ol>

            </div>
        </article>

        <hr />

        <div class="post-info">
  				<p>
  					<svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-tag meta-icon"><path d="M20.59 13.41l-7.17 7.17a2 2 0 0 1-2.83 0L2 12V2h10l8.59 8.59a2 2 0 0 1 0 2.82z"></path><line x1="7" y1="7" x2="7" y2="7"></line></svg><span class="tag"><a href="https://dragarok.github.io/tags/nlp">NLP</a></span><span class="tag"><a href="https://dragarok.github.io/tags/neuralnetwork">neuralnetwork</a></span>
  				</p>
  			</div>

        
    </main>

            </div>

        </div>

        




<script type="text/javascript" src="../../bundle.min.45295cbc2d5bbc09b85caead90108f31defa5c6b650a42abb2e79d59a363e5bc2f11614e43687e66e6d47563560a45c3324b75dcb9b3a724a2955a2f1ce2f114.js" integrity="sha512-RSlcvC1bvAm4XK6tkBCPMd76XGtlCkKrsuedWaNj5bwvEWFOQ2h&#43;ZubUdWNWCkXDMkt13LmzpySilVovHOLxFA=="></script>



    </body>
</html>
