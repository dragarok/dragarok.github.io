<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="ie=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="author" content="[Alok Regmi]">
<meta name="description" content="This criteria distinguishes Whether Interpret-ability is achieved by restricting the complexity of the machine learning model (intrinsic) or by applying methods that analyze the model after training (post hoc)
Permutation feature importance is, for example, a post hoc interpretation method
Contrastive explanation: explaining only the abnormalities Rather than explaining all features, explaining only 3 or 4. Social context &ndash;&gt; who are you going to give answers to
Whether explanation is required or not is also an imp question.
" />
<meta name="keywords" content="Blog, neuralnetwork" />
<meta name="robots" content="noodp" />
<meta name="theme-color" content="" />
<link rel="canonical" href="https://dragarok.github.io/ai/interpretability/" />


    <title>
        
            NN Interpretability ::  
        
    </title>



<link href="https://cdnjs.cloudflare.com/ajax/libs/flag-icon-css/3.2.1/css/flag-icon.min.css" rel="stylesheet"
    type="text/css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">
    
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js" integrity="sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4" crossorigin="anonymous"></script>
    
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js" integrity="sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa" crossorigin="anonymous"
    onload="renderMathInElement(document.body);"></script>





<link rel="stylesheet" href="../../main.min.daa43c14e17ef1950fa593909b64d3a2d9855d65b354823b6d82fca285818e75.css">




    <link rel="apple-touch-icon" sizes="180x180" href="../../apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="../../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../../favicon-16x16.png">
    <link rel="manifest" href="../../site.webmanifest">
    <link rel="mask-icon" href="../../safari-pinned-tab.svg" color="#252627">
    <link rel="shortcut icon" href="../../favicon.ico">
    <meta name="msapplication-TileColor" content="#252627">
    <meta name="theme-color" content="#252627">


  <meta itemprop="name" content="NN Interpretability">
  <meta itemprop="description" content="This criteria distinguishes Whether Interpret-ability is achieved by restricting the complexity of the machine learning model (intrinsic) or by applying methods that analyze the model after training (post hoc)
Permutation feature importance is, for example, a post hoc interpretation method
Contrastive explanation: explaining only the abnormalities Rather than explaining all features, explaining only 3 or 4. Social context –&gt; who are you going to give answers to
Whether explanation is required or not is also an imp question.">
  <meta itemprop="datePublished" content="2020-01-24T00:00:00+00:00">
  <meta itemprop="dateModified" content="2020-05-02T03:40:09+05:45">
  <meta itemprop="wordCount" content="409">
  <meta itemprop="image" content="https://dragarok.github.io/">
  <meta itemprop="keywords" content="Neuralnetwork">
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:image" content="https://dragarok.github.io/">
  <meta name="twitter:title" content="NN Interpretability">
  <meta name="twitter:description" content="This criteria distinguishes Whether Interpret-ability is achieved by restricting the complexity of the machine learning model (intrinsic) or by applying methods that analyze the model after training (post hoc)
Permutation feature importance is, for example, a post hoc interpretation method
Contrastive explanation: explaining only the abnormalities Rather than explaining all features, explaining only 3 or 4. Social context –&gt; who are you going to give answers to
Whether explanation is required or not is also an imp question.">





    <meta property="article:published_time" content="2020-01-24 00:00:00 &#43;0000 UTC" />








    </head>

    <body class="">
        <div class="container">
            <header class="header">
    <span class="header__inner">
        <a href="../../" style="text-decoration: none;">
    <div class="logo">
        
            <span class="logo__mark">~</span>
            <span class="logo__text">K4iv41y4</span>
            <span class="logo__mark">&nbsp;~</span>
            <span class="logo__cursor" style=
                  "
                   
                   ">
            </span>
        
    </div>
</a>


        <span class="header__right">
            
                <nav class="menu">
    <ul class="menu__inner"><li><a href="https://dragarok.github.io/about/">About</a></li><li><a href="https://dragarok.github.io/ai/">AI</a></li><li><a href="https://dragarok.github.io/posts/">Blog</a></li><li><a href="https://dragarok.github.io/books/">Books</a></li><li><a href="https://dragarok.github.io/braindump/">Braindump</a></li>
    </ul>
</nav>

                <span class="menu-trigger">
                    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
                        <path d="M0 0h24v24H0z" fill="none"/>
                        <path d="M3 18h18v-2H3v2zm0-5h18v-2H3v2zm0-7v2h18V6H3z"/>
                    </svg>
                </span>
            

            <span class="theme-toggle unselectable"><svg class="theme-toggler" width="24" height="24" viewBox="0 0 48 48" fill="none" xmlns="http://www.w3.org/2000/svg">
  <path d="M22 41C32.4934 41 41 32.4934 41 22C41 11.5066 32.4934 3 22
  3C11.5066 3 3 11.5066 3 22C3 32.4934 11.5066 41 22 41ZM7 22C7
  13.7157 13.7157 7 22 7V37C13.7157 37 7 30.2843 7 22Z"/>
</svg>
</span>
        </span>
    </span>
</header>


            <div class="content">
                
    <main class="post">

        <div class="post-info">
            
            </p>
        </div>

        <article>
            <h2 class="post-title"><a href="https://dragarok.github.io/ai/interpretability/">NN Interpretability</a></h2>

            

            <div class="post-content">
                <p>This criteria distinguishes Whether Interpret-ability is achieved by restricting the complexity of the machine learning model (intrinsic)
or by applying methods that analyze the model after training (post hoc)</p>
<p>Permutation feature importance is, for example, a post hoc interpretation method</p>
<p>Contrastive explanation: explaining only the abnormalities
Rather than explaining all  features, explaining only 3 or 4.
Social context &ndash;&gt; who are you going to give answers to</p>
<p>Whether explanation is required or not is also an imp question.</p>
<p>An explanation can be portable: applicable from one to other or not.</p>
<p>We tend to ask questions with why this answer and why not that one? That kind of explanation.</p>
<p>People need explanation about results to know how to get different results.</p>
<p>Mostly, the events that have the lower chance of occurence will be more significant.</p>
<p>Fidelity of any explanation is also significant since
for e.g. if model tells adding a balcony increases house&rsquo;s price, it should be applicable
all the time.</p>
<p>Sometimes the model needs to be homoscedastic for all data points meaning have same variance for all
predictions : may not be useful though since houses with higher prices can have more variance compared
to house with low prices.</p>
<p>R squared, ModifiedR squared are used to explain the effect of categorical variables and their variance.
Takes value from 0 to 1.</p>
<p>Feature&rsquo;s weight plot is used to show significance of each feature and their variance.
It can be told from the feature wt plot which features don&rsquo;t impact the result, which does positively
or negatively. Which ones are more random in using for prediction and have lower confidence and so on.</p>
<p>Effects Plot will have an additional contribution multiplied to weights. Similar to normalization
Uses a box plot</p>
<p>Instance Plot:
Now we calculate each feature&rsquo;s contribution and multiply with the instance specific data and find
total contribution to the result. Now the discrepancies in the actual result explanations and  the
current predictions help us understand how single feature can impact the total. P.S. We assumed all
other features being constant while explaining each feature&rsquo;s contribution in the previous case.
So, it&rsquo;s important to do this to understand what kind of features directly impact presence or absence of
other features.
e.g. If a day is rainy , then other good features being true might not contribute much to the original
result that is negative.</p>
<p>Encoding of categorical features:</p>
<p>Several techniques</p>
<p>Techniques determine the significance shown by each category</p>
<p>Treatment coding</p>

            </div>
        </article>

        <hr />

        <div class="post-info">
  				<p>
  					<svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-tag meta-icon"><path d="M20.59 13.41l-7.17 7.17a2 2 0 0 1-2.83 0L2 12V2h10l8.59 8.59a2 2 0 0 1 0 2.82z"></path><line x1="7" y1="7" x2="7" y2="7"></line></svg><span class="tag"><a href="https://dragarok.github.io/tags/neuralnetwork">neuralnetwork</a></span>
  				</p>
  			</div>

        
    </main>

            </div>

        </div>

        




<script type="text/javascript" src="../../bundle.min.74fe699bb673e8362137b575513abfcf73303855d923eea09c0e507deab0ca7f8321880b672790b9e63cc109a18189deebfd13899a8ff536e858791973ffd487.js" integrity="sha512-dP5pm7Zz6DYhN7V1UTq/z3MwOFXZI&#43;6gnA5Qfeqwyn&#43;DIYgLZyeQueY8wQmhgYne6/0TiZqP9TboWHkZc//Uhw=="></script>



    </body>
</html>
