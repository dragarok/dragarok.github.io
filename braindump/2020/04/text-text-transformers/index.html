<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="ie=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="author" content="[Alok Regmi]">
<meta name="description" content="Catches from text to text transfer transformers:
As an example, consider the case of English to German translation: If we have a training datapoint with input sentence “That is good.” and target “Das ist gut.”, we would simply train the model on next-step prediction over the concatenated input sequence “translate English to German: That is good. target: Das ist gut.” If we wanted to obtain the model’s prediction for this example, the model would be fed the prefix “translate English to German: That is good." />
<meta name="keywords" content="Blog, NLP, neuralnetwork" />
<meta name="robots" content="noodp" />
<meta name="theme-color" content="" />
<link rel="canonical" href="https://dragarok.github.io/braindump/2020/04/text-text-transformers/" />


    <title>
        
            Text-Text Transformers :: Alok&#39;s Blog 
        
    </title>



<link href="https://cdnjs.cloudflare.com/ajax/libs/flag-icon-css/3.2.1/css/flag-icon.min.css" rel="stylesheet"
    type="text/css">



<link rel="stylesheet" href="/main.min.d235e788b40b442aba9cd6c69fdc330353b2dec27dcbc4235961469d2155f5a9.css">




    <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
    <link rel="manifest" href="/site.webmanifest">
    <link rel="mask-icon" href="/safari-pinned-tab.svg" color="#252627">
    <link rel="shortcut icon" href="/favicon.ico">
    <meta name="msapplication-TileColor" content="#252627">
    <meta name="theme-color" content="#252627">

<meta itemprop="name" content="Text-Text Transformers">
<meta itemprop="description" content="Catches from text to text transfer transformers:
As an example, consider the case of English to German translation: If we have a training datapoint with input sentence “That is good.” and target “Das ist gut.”, we would simply train the model on next-step prediction over the concatenated input sequence “translate English to German: That is good. target: Das ist gut.” If we wanted to obtain the model’s prediction for this example, the model would be fed the prefix “translate English to German: That is good.">
<meta itemprop="dateModified" content="2020-04-19T15:59:29+05:45" />
<meta itemprop="wordCount" content="328"><meta itemprop="image" content="https://dragarok.github.io/"/>
<meta itemprop="keywords" content="NLP,neuralnetwork," /><meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://dragarok.github.io/"/>

<meta name="twitter:title" content="Text-Text Transformers"/>
<meta name="twitter:description" content="Catches from text to text transfer transformers:
As an example, consider the case of English to German translation: If we have a training datapoint with input sentence “That is good.” and target “Das ist gut.”, we would simply train the model on next-step prediction over the concatenated input sequence “translate English to German: That is good. target: Das ist gut.” If we wanted to obtain the model’s prediction for this example, the model would be fed the prefix “translate English to German: That is good."/>





    <meta property="article:published_time" content="2020-04-19 15:59:29 &#43;0545 &#43;0545" />








    </head>

    <body class="">
        <div class="container">
            <header class="header">
    <span class="header__inner">
        <a href="/" style="text-decoration: none;">
    <div class="logo">
        
            <span class="logo__mark">></span>
            <span class="logo__text">K4iv41y4</span>
            <span class="logo__cursor" style=
                  "
                   
                   ">
            </span>
        
    </div>
</a>


        <span class="header__right">
            
                <nav class="menu">
    <ul class="menu__inner"><li><a href="https://dragarok.github.io/about/">About</a></li><li><a href="https://dragarok.github.io/posts/">Blog</a></li><li><a href="https://dragarok.github.io/books/">Books</a></li><li><a href="https://dragarok.github.io/braindump/">Braindump</a></li>
    </ul>
</nav>

                <span class="menu-trigger">
                    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
                        <path d="M0 0h24v24H0z" fill="none"/>
                        <path d="M3 18h18v-2H3v2zm0-5h18v-2H3v2zm0-7v2h18V6H3z"/>
                    </svg>
                </span>
            

            <span class="theme-toggle unselectable"><svg class="theme-toggler" width="24" height="24" viewBox="0 0 48 48" fill="none" xmlns="http://www.w3.org/2000/svg">
  <path d="M22 41C32.4934 41 41 32.4934 41 22C41 11.5066 32.4934 3 22
  3C11.5066 3 3 11.5066 3 22C3 32.4934 11.5066 41 22 41ZM7 22C7
  13.7157 13.7157 7 22 7V37C13.7157 37 7 30.2843 7 22Z"/>
</svg>
</span>
        </span>
    </span>
</header>


            <div class="content">
                
    <main class="post">

        <div class="post-info">
            <p><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-clock"><circle cx="12" cy="12" r="10"></circle><polyline points="12 6 12 12 16 14"></polyline></svg>2 minutes

            

            </p>
        </div>

        <article>
            <h1 class="post-title">
                <a href="https://dragarok.github.io/braindump/2020/04/text-text-transformers/">Text-Text Transformers</a>
            </h1>

            

            <div class="post-content">
                <p>Catches from text to text transfer transformers:</p>
<ol>
<li>As an example, consider the case of English to German translation: If we have a training</li>
</ol>
<p>datapoint with input sentence “That is good.” and target “Das ist gut.”, we would simply train the
model on next-step prediction over the concatenated input sequence “translate English to German:
That is good. target: Das ist gut.” If we wanted to obtain the model’s prediction for this example,
the model would be fed the prefix “translate English to German: That is good. target:” and would
be asked to generate the remainder of the sequence autoregressively.</p>
<ol>
<li>In order to conquer the disadvantage of causal transformers, the prefix LM model tries to</li>
</ol>
<p>avoid this by making the model to see the sequence non causally during the prefix .
e.g in translating from english to german , it sees whole english sequence but causality is
maintained on generated translation.</p>
<ol>
<li>Lesser the parameters lesser is the training loss: this is explained as the model remembering the</li>
</ol>
<p>training examples rather than really learning to do the task.</p>
<ol>
<li>Repeating pre training data is not always harmful but after a certain limit it is. In case of example of</li>
</ol>
<p>C4 database with 2^35 tokens, when using dataset with 2^29 tokens, we can repeat this by 2^6 times means 64. Till
that much the performance doesn&rsquo;t deteriorate.</p>
<ol>
<li>
<p>artificial “limit”</p>
</li>
<li>
<p>Temperature specific mixing</p>
</li>
<li>
<p>We find that fine-tuning after multi-task</p>
</li>
</ol>
<p>pre-training results in comparable performance to our baseline.</p>
<ol>
<li>Interestingly, the performance of “leave-one-out” training was only slightly</li>
</ol>
<p>worse, suggesting that a model that was trained on a variety of tasks can still adapt to new tasks (i.e.
multi-task pre-training might not result in a dramatic task interference)</p>
<ol>
<li>Instead of pre-training 4 separate models, a cheaper alternative is to take</li>
</ol>
<p>a single pre-trained model and produce 4 separate fine-tuned versions</p>
<ol>
<li>Separately, we note that ensembling N separate models</li>
</ol>
<p>has a similar cost to using a model that has an N× higher computational cost.</p>
<ol>
<li>
<p>Language agnostic models</p>
</li>
<li></li>
</ol>

            </div>
        </article>

        <hr />

        <div class="post-info">
                <p>
                    <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-tag meta-icon"><path d="M20.59 13.41l-7.17 7.17a2 2 0 0 1-2.83 0L2 12V2h10l8.59 8.59a2 2 0 0 1 0 2.82z"></path><line x1="7" y1="7" x2="7" y2="7"></line></svg><span class="tag"><a href="https://dragarok.github.io/tags/nlp">NLP</a></span><span class="tag"><a href="https://dragarok.github.io/tags/neuralnetwork">neuralnetwork</a></span>
                </p>

            <p><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-file-text"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>328 Words</p>

            <p><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-calendar"><rect x="3" y="4" width="18" height="18" rx="2" ry="2"></rect><line x1="16" y1="2" x2="16" y2="6"></line><line x1="8" y1="2" x2="8" y2="6"></line><line x1="3" y1="10" x2="21" y2="10"></line></svg>2020-04-19 15:59 &#43;0545</p>
        </div>

        
            <div class="pagination">
                <div class="pagination__title">
                    <span class="pagination__title-h">Read other post</span>
                    <hr />
                </div>

                <div class="pagination__buttons">
                    
                        <span class="button previous">
                            <a href="https://dragarok.github.io/braindump/2020/04/paper-reading-3-pass/">
                                <span class="button__icon">←</span>
                                <span class="button__text">Paper Reading 3 pass</span>
                            </a>
                        </span>
                    

                    
                        <span class="button next">
                            <a href="https://dragarok.github.io/braindump/2020/04/ra-and-bi/">
                                <span class="button__text">RA AND BI</span>
                                <span class="button__icon">→</span>
                            </a>
                        </span>
                    
                </div>
            </div>
        

        
    </main>

            </div>

            
                <footer class="footer">
    <div class="footer__inner">
        <div class="footer__content">
            <span>&copy; 2023</span>
            
                <span><a href="https://dragarok.github.io/">Alok Regmi</a></span>
            
            <span>Theme by <a href="https://github.com/rhazdon">Djordje Atlialp</a></span>
        </div>
    </div>
</footer>

            
        </div>

        




<script type="text/javascript" src="/bundle.min.fbf016a5f89be88f4916a0898302f777692ee6600991c3b6108fc423ed982dc83b470742cc92dd796861136f5ca96988b35e049b5f4cb4b1343686784a58c8ce.js" integrity="sha512-&#43;/AWpfib6I9JFqCJgwL3d2ku5mAJkcO2EI/EI&#43;2YLcg7RwdCzJLdeWhhE29cqWmIs14Em19MtLE0NoZ4SljIzg=="></script>



    </body>
</html>
