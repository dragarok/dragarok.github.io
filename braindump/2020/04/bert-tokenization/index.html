<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="ie=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="author" content="[Alok Regmi]">
<meta name="description" content="&quot;&quot;&quot; Adapted from Oliver Atanaszov&rsquo;s notebook on transformer fine-tuning https://github.com/ben0it8/containerized-transformer-finetuning/blob/develop/research/finetune-transformer-on-imdb5k.ipynb &quot;&quot;&quot; from concurrent.futures import ProcessPoolExecutor import multiprocessing import os import numpy as np import pandas as pd import torch from torch.utils.data import TensorDataset, random_split, DataLoader from tqdm import tqdm
n_cpu = multiprocessing.cpu_count() MAX_LENGTH = 256
class TextProcessor: def __init__(self, tokenizer, label2id: dict, max_length: int=512): self.tokenizer = tokenizer self.label2id = label2id self.max_length = max_length self.clf_token = self.tokenizer.vocab[&rsquo;[CLS]&rsquo;] self.pad_token = self.tokenizer.vocab[&rsquo;[PAD]&#39;]
def encode(self, input): return list(self.tokenizer.convert_tokens_to_ids(o) for o in input)
" />
<meta name="keywords" content="Blog, tokenization, bert" />
<meta name="robots" content="noodp" />
<meta name="theme-color" content="" />
<link rel="canonical" href="https://dragarok.github.io/braindump/2020/04/bert-tokenization/" />


    <title>
        
            Bert Tokenization ::  
        
    </title>



<link href="https://cdnjs.cloudflare.com/ajax/libs/flag-icon-css/3.2.1/css/flag-icon.min.css" rel="stylesheet"
    type="text/css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">
    
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js" integrity="sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4" crossorigin="anonymous"></script>
    
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js" integrity="sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa" crossorigin="anonymous"
    onload="renderMathInElement(document.body);"></script>





<link rel="stylesheet" href="../../../../main.min.b0d234ca364f76d18f74dd789a642d763e058fe48a746787387c0cdd21815e04.css">




    <link rel="apple-touch-icon" sizes="180x180" href="../../../../apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="../../../../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../../../../favicon-16x16.png">
    <link rel="manifest" href="../../../../site.webmanifest">
    <link rel="mask-icon" href="../../../../safari-pinned-tab.svg" color="#252627">
    <link rel="shortcut icon" href="../../../../favicon.ico">
    <meta name="msapplication-TileColor" content="#252627">
    <meta name="theme-color" content="#252627">


  <meta itemprop="name" content="Bert Tokenization">
  <meta itemprop="description" content="&#34;&#34;&#34; Adapted from Oliver Atanaszov’s notebook on transformer fine-tuning https://github.com/ben0it8/containerized-transformer-finetuning/blob/develop/research/finetune-transformer-on-imdb5k.ipynb &#34;&#34;&#34; from concurrent.futures import ProcessPoolExecutor import multiprocessing import os import numpy as np import pandas as pd import torch from torch.utils.data import TensorDataset, random_split, DataLoader from tqdm import tqdm
n_cpu = multiprocessing.cpu_count() MAX_LENGTH = 256
class TextProcessor: def __init__(self, tokenizer, label2id: dict, max_length: int=512): self.tokenizer = tokenizer self.label2id = label2id self.max_length = max_length self.clf_token = self.tokenizer.vocab[’[CLS]’] self.pad_token = self.tokenizer.vocab[’[PAD]&#39;]
def encode(self, input): return list(self.tokenizer.convert_tokens_to_ids(o) for o in input)">
  <meta itemprop="datePublished" content="2020-04-19T15:59:25+05:45">
  <meta itemprop="dateModified" content="2020-04-19T15:59:25+05:45">
  <meta itemprop="wordCount" content="255">
  <meta itemprop="image" content="https://dragarok.github.io/">
  <meta itemprop="keywords" content="Tokenization,Bert">
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:image" content="https://dragarok.github.io/">
  <meta name="twitter:title" content="Bert Tokenization">
  <meta name="twitter:description" content="&#34;&#34;&#34; Adapted from Oliver Atanaszov’s notebook on transformer fine-tuning https://github.com/ben0it8/containerized-transformer-finetuning/blob/develop/research/finetune-transformer-on-imdb5k.ipynb &#34;&#34;&#34; from concurrent.futures import ProcessPoolExecutor import multiprocessing import os import numpy as np import pandas as pd import torch from torch.utils.data import TensorDataset, random_split, DataLoader from tqdm import tqdm
n_cpu = multiprocessing.cpu_count() MAX_LENGTH = 256
class TextProcessor: def __init__(self, tokenizer, label2id: dict, max_length: int=512): self.tokenizer = tokenizer self.label2id = label2id self.max_length = max_length self.clf_token = self.tokenizer.vocab[’[CLS]’] self.pad_token = self.tokenizer.vocab[’[PAD]&#39;]
def encode(self, input): return list(self.tokenizer.convert_tokens_to_ids(o) for o in input)">





    <meta property="article:published_time" content="2020-04-19 15:59:25 &#43;0545 &#43;0545" />







<script async src="https://www.googletagmanager.com/gtag/js?id=G-TYZR3W5F57"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-TYZR3W5F57');
</script>


    </head>

    <body class="">
        <div class="container">
            <header class="header">
    <span class="header__inner">
        <a href="../../../../" style="text-decoration: none;">
    <div class="logo">
        
            <span class="logo__mark">~</span>
            <span class="logo__text">K4iv41y4</span>
            <span class="logo__mark">&nbsp;~</span>
            <span class="logo__cursor" style=
                  "
                   
                   ">
            </span>
        
    </div>
</a>


        <span class="header__right">
            
                <nav class="menu">
    <ul class="menu__inner"><li><a href="https://dragarok.github.io/about/">About</a></li><li><a href="https://dragarok.github.io/ai/">AI</a></li><li><a href="https://dragarok.github.io/posts/">Blog</a></li><li><a href="https://dragarok.github.io/books/">Books</a></li><li><a href="https://dragarok.github.io/braindump/">Braindump</a></li>
    </ul>
</nav>

                <span class="menu-trigger">
                    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
                        <path d="M0 0h24v24H0z" fill="none"/>
                        <path d="M3 18h18v-2H3v2zm0-5h18v-2H3v2zm0-7v2h18V6H3z"/>
                    </svg>
                </span>
            

            <span class="theme-toggle unselectable"><svg class="theme-toggler" width="24" height="24" viewBox="0 0 48 48" fill="none" xmlns="http://www.w3.org/2000/svg">
  <path d="M22 41C32.4934 41 41 32.4934 41 22C41 11.5066 32.4934 3 22
  3C11.5066 3 3 11.5066 3 22C3 32.4934 11.5066 41 22 41ZM7 22C7
  13.7157 13.7157 7 22 7V37C13.7157 37 7 30.2843 7 22Z"/>
</svg>
</span>
        </span>
    </span>
</header>


            <div class="content">
                
    <main class="post">

        <div class="post-info">
            <p><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-clock"><circle cx="12" cy="12" r="10"></circle><polyline points="12 6 12 12 16 14"></polyline></svg>2 minutes

            

            </p>
        </div>

        <article>
            <h1 class="post-title">
                <a href="https://dragarok.github.io/braindump/2020/04/bert-tokenization/">Bert Tokenization</a>
            </h1>

            

            <div class="post-content">
                <p>&quot;&quot;&quot;
Adapted from Oliver Atanaszov&rsquo;s notebook on transformer fine-tuning
<a href="https://github.com/ben0it8/containerized-transformer-finetuning/blob/develop/research/finetune-transformer-on-imdb5k.ipynb">https://github.com/ben0it8/containerized-transformer-finetuning/blob/develop/research/finetune-transformer-on-imdb5k.ipynb</a>
&quot;&quot;&quot;
from concurrent.futures import ProcessPoolExecutor
import multiprocessing
import os
import numpy as np
import pandas as pd
import torch
from torch.utils.data import TensorDataset, random_split, DataLoader
from tqdm import tqdm</p>
<p>n_cpu = multiprocessing.cpu_count()
MAX_LENGTH = 256</p>
<p>class TextProcessor:
def __init__(self, tokenizer, label2id: dict, max_length: int=512):
self.tokenizer = tokenizer
self.label2id = label2id
self.max_length = max_length
self.clf_token = self.tokenizer.vocab[&rsquo;[CLS]&rsquo;]
self.pad_token = self.tokenizer.vocab[&rsquo;[PAD]']</p>
<p>def encode(self, input):
return list(self.tokenizer.convert_tokens_to_ids(o) for o in input)</p>
<p>def token2id(self, item: Tuple[str, str]):
&ldquo;Convert text (item[0]) to sequence of IDs and label (item[1]) to integer&rdquo;
assert len(item) == 2   # Need a row of text AND labels
label, text = item[0], item[1]
assert isinstance(text, str)   # Need position 1 of input to be of type(str)
inputs = self.tokenizer.tokenize(text)</p>
<p>if len(inputs) &gt;= self.max_length:
inputs = inputs[:self.max_length - 1]
ids = self.encode(inputs) + [self.clf_token]
else:
pad = [self.pad_token] * (self.max_length - len(inputs) - 1)
ids = self.encode(inputs) + [self.clf_token] + pad</p>
<p>return np.array(ids, dtype=&lsquo;int64&rsquo;), self.label2id[label]</p>
<p>def process_row(self, row):
&ldquo;Calls the token2id method of the text processor for passing items to executor&rdquo;
return self.token2id((row[1][LABEL_COL], row[1][TEXT_COL]))</p>
<p>def create_dataloader(self,
df: pd.DataFrame,
batch_size: int = 32,
shuffle: bool = False,
valid_pct: float = None):
&ldquo;Process rows in pd.DataFrame using n_cpus and return a DataLoader&rdquo;</p>
<p>tqdm.pandas()
with ProcessPoolExecutor(max_workers=n_cpu) as executor:
result = list(
tqdm(executor.map(self.process_row, df.iterrows(), chunksize=8192),
desc=f&quot;Processing {len(df)} examples on {n_cpu} cores&quot;,
total=len(df)))</p>
<p>features = [r[0] for r in result]
labels = [r[1] for r in result]</p>
<p>dataset = TensorDataset(torch.tensor(features, dtype=torch.long),
torch.tensor(labels, dtype=torch.long))</p>
<p>data_loader = DataLoader(dataset,
batch_size=batch_size,
num_workers=0,
shuffle=shuffle,
pin_memory=torch.cuda.is_available())
return data_loader</p>

            </div>
        </article>

        <hr />

        <div class="post-info">
                <p>
                    <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-tag meta-icon"><path d="M20.59 13.41l-7.17 7.17a2 2 0 0 1-2.83 0L2 12V2h10l8.59 8.59a2 2 0 0 1 0 2.82z"></path><line x1="7" y1="7" x2="7" y2="7"></line></svg><span class="tag"><a href="https://dragarok.github.io/tags/tokenization">tokenization</a></span><span class="tag"><a href="https://dragarok.github.io/tags/bert">bert</a></span>
                </p>

            <p><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-file-text"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>255 Words</p>

            <p><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-calendar"><rect x="3" y="4" width="18" height="18" rx="2" ry="2"></rect><line x1="16" y1="2" x2="16" y2="6"></line><line x1="8" y1="2" x2="8" y2="6"></line><line x1="3" y1="10" x2="21" y2="10"></line></svg>2020-04-19 15:59 &#43;0545</p>
        </div>

        
            <div class="pagination">
                <div class="pagination__title">
                    <span class="pagination__title-h">Read other post</span>
                    <hr />
                </div>

                <div class="pagination__buttons">
                    
                        <span class="button previous">
                            <a href="https://dragarok.github.io/braindump/2020/04/sqlbasics/">
                                <span class="button__icon">←</span>
                                <span class="button__text">SQLBasics</span>
                            </a>
                        </span>
                    

                    
                        <span class="button next">
                            <a href="https://dragarok.github.io/braindump/2020/04/euler-621/">
                                <span class="button__text">Euler 621</span>
                                <span class="button__icon">→</span>
                            </a>
                        </span>
                    
                </div>
            </div>
        

        
    </main>

            </div>

        </div>

        




<script type="text/javascript" src="../../../../bundle.min.74fe699bb673e8362137b575513abfcf73303855d923eea09c0e507deab0ca7f8321880b672790b9e63cc109a18189deebfd13899a8ff536e858791973ffd487.js" integrity="sha512-dP5pm7Zz6DYhN7V1UTq/z3MwOFXZI&#43;6gnA5Qfeqwyn&#43;DIYgLZyeQueY8wQmhgYne6/0TiZqP9TboWHkZc//Uhw=="></script>



    </body>
</html>
