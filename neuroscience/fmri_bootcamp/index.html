<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="ie=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="author" content="[Alok Regmi]">
<meta name="description" content="Different views of the brain Coronal view: View after cutting the face off. Sagittal medial view: Cutting brain from the sides into the middle. Lateral Sagittal view: Take the skull off. Axial view: Look from the top after cutting off a piece. Sulca: Bottom of cortical folds Gyrus: Top of cortical folds Different Imaging Techniques Anatomical Imaging Axial slices are 2D representations. Making multiple axial slices &amp;ndash;&amp;gt; 3D representation of the brain Slices can be separated by some space." />
<meta name="keywords" content="Blog" />
<meta name="robots" content="noodp" />
<meta name="theme-color" content="" />
<link rel="canonical" href="https://dragarok.github.io/neuroscience/fmri_bootcamp/" />


    <title>
        
            FMRI bootcamp ::  
        
    </title>



<link href="https://cdnjs.cloudflare.com/ajax/libs/flag-icon-css/3.2.1/css/flag-icon.min.css" rel="stylesheet"
    type="text/css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">
    
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js" integrity="sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4" crossorigin="anonymous"></script>
    
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js" integrity="sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa" crossorigin="anonymous"
    onload="renderMathInElement(document.body);"></script>





<link rel="stylesheet" href="../../main.min.47d1ac1539fe1320a4309408fbd7b3a0a50b48482799b1e81ee0115e02cb24e2.css">




    <link rel="apple-touch-icon" sizes="180x180" href="../../apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="../../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../../favicon-16x16.png">
    <link rel="manifest" href="../../site.webmanifest">
    <link rel="mask-icon" href="../../safari-pinned-tab.svg" color="#252627">
    <link rel="shortcut icon" href="../../favicon.ico">
    <meta name="msapplication-TileColor" content="#252627">
    <meta name="theme-color" content="#252627">


  <meta itemprop="name" content="FMRI bootcamp">
  <meta itemprop="description" content="Different views of the brain Coronal view: View after cutting the face off. Sagittal medial view: Cutting brain from the sides into the middle. Lateral Sagittal view: Take the skull off. Axial view: Look from the top after cutting off a piece. Sulca: Bottom of cortical folds Gyrus: Top of cortical folds Different Imaging Techniques Anatomical Imaging Axial slices are 2D representations. Making multiple axial slices –&gt; 3D representation of the brain Slices can be separated by some space.">
  <meta itemprop="datePublished" content="2020-08-13T00:00:00+05:45">
  <meta itemprop="dateModified" content="2020-08-29T17:35:09+05:45">
  <meta itemprop="wordCount" content="1939">
  <meta itemprop="image" content="https://dragarok.github.io/">
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:image" content="https://dragarok.github.io/">
  <meta name="twitter:title" content="FMRI bootcamp">
  <meta name="twitter:description" content="Different views of the brain Coronal view: View after cutting the face off. Sagittal medial view: Cutting brain from the sides into the middle. Lateral Sagittal view: Take the skull off. Axial view: Look from the top after cutting off a piece. Sulca: Bottom of cortical folds Gyrus: Top of cortical folds Different Imaging Techniques Anatomical Imaging Axial slices are 2D representations. Making multiple axial slices –&gt; 3D representation of the brain Slices can be separated by some space.">





    <meta property="article:published_time" content="2020-08-13 00:00:00 &#43;0545 &#43;0545" />








    </head>

    <body class="">
        <div class="container">
            <header class="header">
    <span class="header__inner">
        <a href="../../" style="text-decoration: none;">
    <div class="logo">
        
            <span class="logo__mark">~</span>
            <span class="logo__text">K4iv41y4</span>
            <span class="logo__mark">&nbsp;~</span>
            <span class="logo__cursor" style=
                  "
                   
                   ">
            </span>
        
    </div>
</a>


        <span class="header__right">
            
                <nav class="menu">
    <ul class="menu__inner"><li><a href="https://dragarok.github.io/about/">About</a></li><li><a href="https://dragarok.github.io/ai/">AI</a></li><li><a href="https://dragarok.github.io/posts/">Blog</a></li><li><a href="https://dragarok.github.io/books/">Books</a></li><li><a href="https://dragarok.github.io/braindump/">Braindump</a></li>
    </ul>
</nav>

                <span class="menu-trigger">
                    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
                        <path d="M0 0h24v24H0z" fill="none"/>
                        <path d="M3 18h18v-2H3v2zm0-5h18v-2H3v2zm0-7v2h18V6H3z"/>
                    </svg>
                </span>
            

            <span class="theme-toggle unselectable"><svg class="theme-toggler" width="24" height="24" viewBox="0 0 48 48" fill="none" xmlns="http://www.w3.org/2000/svg">
  <path d="M22 41C32.4934 41 41 32.4934 41 22C41 11.5066 32.4934 3 22
  3C11.5066 3 3 11.5066 3 22C3 32.4934 11.5066 41 22 41ZM7 22C7
  13.7157 13.7157 7 22 7V37C13.7157 37 7 30.2843 7 22Z"/>
</svg>
</span>
        </span>
    </span>
</header>


            <div class="content">
                
    <main class="post">

        <div class="post-info">
            
            </p>
        </div>

        <article>
            <h2 class="post-title"><a href="https://dragarok.github.io/neuroscience/fmri_bootcamp/">FMRI bootcamp</a></h2>

            

            <div class="post-content">
                <h2 id="different-views-of-the-brain">Different views of the brain</h2>
<ol>
<li>Coronal view: View after cutting the face off.</li>
<li>Sagittal medial view: Cutting brain from the sides into the middle.</li>
<li>Lateral Sagittal view: Take the skull off.</li>
<li>Axial view: Look from the top after cutting off a piece.</li>
</ol>
<p><img src="../../ox-hugo/b57bac4e4c9e0708d7855f9752d6074f8b96ab43.svg">
<img src="../../ox-hugo/1d91c9ba6831f8332f47a09ab2c5f1418141f44a.svg"></p>
<ul>
<li>Sulca: Bottom of cortical folds</li>
<li>Gyrus: Top of cortical folds</li>
</ul>
<h2 id="different-imaging-techniques">Different Imaging Techniques</h2>
<h3 id="anatomical-imaging">Anatomical Imaging</h3>
<ul>
<li>Axial slices are 2D representations.</li>
<li>Making multiple axial slices &ndash;&gt; 3D representation of the brain</li>
<li>Slices can be separated by some space.</li>
<li>Voxel: Spatial Resolution of 3D fMRI representation</li>
<li>In plane resolution: Pixel of 2D slice
<ul>
<li>Going very high resolution has a downside that the blood flow in the brain
decreases SNR in our images.</li>
<li>Dead brains can be imaged with very high resolution.</li>
</ul>
</li>
<li>Spatial Resolution depends on pixel and separation.</li>
<li>Example: 1 mm isotropic , 1*1*1 mm, 176 slices, 5 min : Example of how fMRI
are defined.</li>
</ul>
<h3 id="functional-imaging">Functional Imaging</h3>
<ul>
<li>Add temporal resolution to anatomical data. The 3D representation taken in
different time steps. The time between two images is known as repetition time.
Generally it&rsquo;s 2 seconds.</li>
<li>2 seconds vs 5 minutes: Sacrifice spatial resolution; Generally it&rsquo;s 3*3*3 and
30 slices.</li>
<li>27 anatomical voxels compared to 1 anatomical voxel.</li>
</ul>
<h2 id="fmri-time-course">fMRI Time-course</h2>
<h3 id="hemodynamic-response-function--hrf">Hemodynamic Response function(HRF)</h3>
<figure><img src="../../ox-hugo/05215436aceef7c091f443ffa5ca06e1e1791055.svg">
</figure>

<ul>
<li>The brain has a high oxygen level in the region after activity. There is a
undershoot after some time after peak.</li>
<li>This is the general HRF used to process fMRI data.</li>
<li>HRF looked in baby mice was found to be very slow. Mostly for adults and
general experiments, the standard HRF is used. α is the time to peak
which is also called Human Anatomical Lag and β is the peak level. α
is fixed generally and we solve for β.</li>
<li>Variability in α is smaller than β. So, even if we have different
values of α across different brain regions, we neglect it for general experiments.</li>
<li>2 seconds time in Functional fMRI is only useful for the HRF just drawn above.
So, we need to have it small to resolve the peak of HRF.</li>
<li>Big blood vessels at smaller scale can turn into noise around the given voxel.</li>
<li>Astrocyte in early visual cortex have 1:1 relation with neurons and can be
used to get same spatial resolution as the neural activity.</li>
<li>The shape of HRF and α is something we take as prior knowledge.</li>
<li>Negative β is when given the input signal, the blood oxygen level
decreases relative to normal case.</li>
<li>BLOD signal [ Actual signal ] Unit:
<ul>
<li>%SC[Signal Change]: Relative to normal, how different is
BLOD signal in the experiment.</li>
</ul>
</li>
</ul>
<h3 id="general-linear-model--glms">General Linear Model (GLMs)</h3>
<ul>
<li>Boxcar regressor : Box shaped signal multiplied by HRF signal.</li>
<li>Single GLM with multiple regressors.</li>
<li>β values per voxel is taken. So, it&rsquo;s a single 3D image of brain with
different beta. If you have multiple inputs, there will be multiple β
values. Or you can have β count = no of runs * no of conditions</li>
<li>If you just have β image and don&rsquo;t know how many runs did β came from one
signal&rsquo;s full time snapshot, then it&rsquo;s ruined data. You need information about
how the data&rsquo;s obtained as well.</li>
</ul>
<h3 id="noise">Noise:</h3>
<ul>
<li>e.g Sneezing where the whole brain breaks and has spike in
response at that time. People can move their head and these noise make the
signal unusual or crazy.</li>
<li><strong>Solution:</strong> To handle this, put an impulse regressor with single
time-point with same height as the data&rsquo;s amplitude. So, this regressor
explains whole information and no other regressors don&rsquo;t take into account of
it since the first one explains it by default.</li>
</ul>
<h3 id="steps-in-data-analysis">Steps in data analysis:</h3>
<ul>
<li>Raw data: Scanner in DICOOM format.</li>
<li>Preprocessing: Motion correction of head, smoothing etc.</li>
<li>Modeling: Comparing the data with your prior and your input. The output will
be β values.</li>
</ul>
<h2 id="univariate-analysis">Univariate analysis</h2>
<h3 id="answer-how-much-is-neural-activity">Answer <strong>How much</strong> is neural activity?</h3>
<ul>
<li>Need to first answer <strong>Where</strong> question first.</li>
<li>Average/Any other metric  of all voxels in the region of interest.  The
difference of betas between different ROIs is known as <strong>contrast value</strong>.</li>
<li>Even though we can&rsquo;t ask <strong>contrast value</strong>&rsquo;s quantity means but we can see if
there is a difference in contrast value across different persons.</li>
<li>So, this can tell only the <strong>consistency</strong> of the data using <strong>t-test.</strong></li>
</ul>
<h3 id="answer-where-is-the-neural-activity">Answer <strong>Where</strong> is the neural activity?</h3>
<ul>
<li>You don&rsquo;t know where you should find the activity.</li>
<li>Now for every voxel, <strong>contrast value</strong> is calculated obtaining a 3D map of brain
of contrast values.</li>
<li>Know the variance of each voxel data from initial beta calculation analysis.</li>
<li><strong>t-map</strong> has value at every voxel which is size of <strong>contrast value</strong> compared to
the <strong>unexplained variance</strong> used in calculating initial beta.</li>
<li>Now find <strong>big values</strong> in t-map using <strong>thresholding</strong>. Generally threshold is
&lt;0.001. Now, we generally get contiguous voxels as our region. Now, we can use
this to answer <strong>How much</strong> question now.</li>
</ul>
<h3 id="whole-brain-group-analysis">Whole Brain Group Analysis</h3>
<pre><code>ID: 827294d7-0097-4aac-a0fc-9b29c6b836d7
</code></pre>
<ol>
<li>Align brains
<ol>
<li>How do you know which voxels are the same in different person?</li>
<li>MNI-152 brain. Average <strong>anatomical template</strong> of brain.</li>
<li>Most normalizing images are the functional ones since they are the ones
that need aligning. So, we <strong>register</strong> functional mapping to it&rsquo;s analytical
image and then map it to anatomical template. The first mapping is pretty
easier compared to the second. These steps are known as <strong>Registration</strong> and
<strong>Normalization</strong>.</li>
<li>Figure out best fit between the images using affine transformations and
calculate which point in anatomical brain is similar in the template.
This is not perfect. For higher regions, the alignment is very poor. Most
easier to map is V1 and it&rsquo;s also only 40% accurate.</li>
<li><a href="../../neuroscience/fmri_hyperalignment/">FMRI Hyperalignment</a> and Functional ROI are other steps that can be added to
solve the problem with the mapping talked above.</li>
</ol>
</li>
<li>Statistics over people
<ol>
<li><strong>Contrast value</strong> across different person and for specific region, we can
average <strong>contrast value</strong> for all persons. Now, for every <strong>contrast map</strong> across
persons, <strong>t-map</strong> by using <strong>variability</strong> across persons. Then, <strong>threshold</strong> t-map.
This statistics over people is known as <strong>Second level Analysis/Random</strong>
<strong>Effects Analysis</strong>. For this analysis, the number of people will define the
robustness of experiment.</li>
</ol>
</li>
</ol>
<h2 id="multivariate-analysis">Multivariate analysis</h2>
<ul>
<li>Comparing to the univariate analysis, where we draw different betas across
voxels distributed in space for different conditions, we know which voxels
activate to which signal. This was the previous way of analysing our signal.</li>
<li>Now, we can represent all voxels as dimension of multidimensional space. If we
have 12 voxels, it&rsquo;s 12 dimensions. But, we can&rsquo;t observe spatial relation
between voxels though. We cannot do such analysis with all voxels though.</li>
<li><strong>Which voxels to choose?</strong></li>
<li><strong>Advantages</strong>
<ul>
<li>Classification</li>
<li>Similarity Analysis using Distance</li>
</ul>
</li>
</ul>
<h2 id="relationship">Relationship</h2>
<ul>
<li>Brain region in superior temporal gyrus. &ldquo;TO RUN&rdquo; vs &ldquo;THE ROCK&rdquo;, the words
have different activity in this region. What is this about? <strong>Verbs vs Nouns</strong> or
<strong>Objects in motion vs Stationary Objects</strong>. This is about testing the question.</li>
<li>Experimented with more pair of words drawn from four classes:
<ul>
<li>Verbs with motion : &ldquo;To run&rdquo;</li>
<li>Verbs without motion: &ldquo;To think&rdquo;</li>
<li>Nouns with motion: &ldquo;The Tiger&rdquo;</li>
<li>Nouns without motion: &ldquo;The Rock&rdquo;</li>
</ul>
</li>
<li>Experimental data:
<ul>
<li>Prior Analysis
<ul>
<li>Time series analysis</li>
<li>1 regressor per condition per run</li>
<li>8 runs in the experiment</li>
<li>4 different conditions</li>
<li><strong>Total of 32 betas for each voxel found</strong></li>
</ul>
</li>
</ul>
</li>
<li>Univariate analysis:
<ul>
<li>Contrast Image
<ul>
<li>Running vs Rock in some person which is above the threshold. This is the
region of interest.</li>
<li>We do it for different persons. Now, we say &ldquo;STG&rdquo; is actually where the
differences occur.</li>
</ul>
</li>
<li><strong>Now , univariate question &ldquo;How much&rdquo; ?</strong>
<ul>
<li>Averaging over runs. Now, we can have &ldquo;RUN&rdquo; vs &ldquo;ROCK&rdquo; difference of betas.
We can do it similarly for all sets of differences between conditions:
&ldquo;nouns vs verbs&rdquo; or &ldquo;motion vs stationary&rdquo;. The differences should be
larger on the criteria that we intend to find.</li>
<li>Now, though you find a distinction, there might be different set of
differences between the nouns you are taking as well. e.g. &ldquo;The Tiger&rdquo; and
&ldquo;The Rock&rdquo; might be interpreted into different classes as well. So, this
goes on. Philosophical debate on &ldquo;non-zero&rdquo; responses in all conditions.
What does it mean?</li>
</ul>
</li>
</ul>
</li>
<li>Multivariate analysis:
<ul>
<li>Now, In ROI of around 80 voxels, 8 beta values per run in each condition.</li>
<li>In each subject, this can be plotted separately. &ldquo;Leave one out&rdquo; beta
classification with two boundaries: &ldquo;ROCK and TIGER&rdquo; vs &ldquo;RUNNING&rdquo; and
&ldquo;THINKING&rdquo;.  Now, can you classify between nouns and verbs inside the
multivariate analysis. You plot seven betas thus and make boundary. Finally
you average all those results.</li>
<li><strong>Think about it</strong> : If we see difference in univariate analysis, there will
surely be difference in multivariate analysis as well. But, even if there is
no difference in univariate analysis , there might be difference that could
be observed in multivariate analysis.
<ul>
<li>About the example: We know &ldquo;NOUNS&rdquo; and &ldquo;VERBS&rdquo; were distinctly separated
in univariate analysis. So, we can separate that boundary clearly in
multivariate as well. But, can we separate &ldquo;MOTION NOUNS&rdquo; vs &ldquo;STATIONARY
NOUNS&rdquo; and similarly for verbs is a question that can be answered by
multivariate analysis only.</li>
</ul>
</li>
<li>Relative information is one that is available in multivariate analysis only.</li>
<li>Classification accuracy in verbs and nouns can now be found. This can show
now that the &ldquo;STG&rdquo; region also has more information to classify between the
nouns or the verbs if the difference in accuracy is relatively high.</li>
</ul>
</li>
</ul>
<h2 id="finding-useful-region">Finding useful region</h2>
<ul>
<li><strong>ROI Identification</strong> in multivariate analysis. <strong>Where question</strong></li>
<li>Commonly known as <strong>Feature Selection</strong> in ML where the voxels are the features.</li>
<li>Choosing Features:
<ul>
<li>Anatomy  : e.g Amygdala</li>
<li>Functional area e.g. SFG [Contiguous regions]</li>
<li>Functional PCI-n : Number of voxels in any direction.</li>
</ul>
</li>
<li><strong>Why choose voxels?</strong>
<ul>
<li>Relevant unit of analysis
<ul>
<li>Local region: Region with certain characteristic that carries out a
functional work or encode certain feature space.</li>
<li>Functionally defined network: Even if regions are not contiguous, they
might have functional correlation. e.g. Regions in opposite hemispheres.</li>
</ul>
</li>
<li>Statistical Reasons
<ul>
<li>By adding noise i.e. not useful voxels to the total space, we will have low
SNR: so lower quality of analysis. Thus, we choose only a number of voxels
for our analysis.</li>
<li>Noise also can create significant differences in signal. So, if we know
relevant voxels, we can ignore voxels that do not help in our analysis. It
prevents over-fitting in general.</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="classification">Classification</h2>
<pre><code>ID: acb8de53-9d7b-4375-add1-d7a5dd14ba8e
</code></pre>
<ul>
<li>Example:
<ul>
<li>Show female sad faces and happy faces. Now, classify them each run, each
individual and finally average them. This is the first step.</li>
<li>We want to know if the emotions in real are making the difference in signal,
if you show male sad faces and happy faces; if the original boundary
generalizes to male faces as well, you know that emotions is encoded by that
region.</li>
<li>For number of runs required per person, there is not a fixed number but it&rsquo;s
generally good to have more runs.</li>
</ul>
</li>
<li>In binary classification <strong>NEVER</strong> have unequal number of images in each run.
Don&rsquo;t let bias get into it.</li>
<li>In data, we need to take care of the additional information present in the
data as well.</li>
<li>The overall output was that there was proof for certain region which
classified general images but could not generalize to cartoon images. And
there was proof for different region that could generalize to both regions.
So, one implies only facial information present whereas the other implies more
to all regions.</li>
<li>Generally, you look for main information by comparing different brain regions.
If you see a likely result from that classification, then there is a
hierarchical number of analyses that can be done on top of that result.</li>
<li>Do positive control such as : Rotation of images and so on. You can do
negative control as well.</li>
<li><strong>Near Generalization :</strong> Generalizing to man faces in this case.</li>
<li><strong>Far Generalization</strong>: Generalizing to cartoon faces as well.</li>
</ul>
<p><a href="../../neuroscience/representational_similarity_analysis/">Representational Similarity Analysis</a></p>
<p><a href="../../neuroscience/fmri_comparisons/">FMRI Comparisons</a></p>

            </div>
        </article>

        <hr />

        <div class="post-info">
  			</div>

        
    </main>

            </div>

        </div>

        




<script type="text/javascript" src="../../bundle.min.329ee33333b8e75309a69907c5873b8594382eb187cf4f064303561f90286018c382e7c682dfd9eeec7f0fd66763f4fab24b1f37b2a918f5f0087f9bda761a26.js" integrity="sha512-Mp7jMzO451MJppkHxYc7hZQ4LrGHz08GQwNWH5AoYBjDgufGgt/Z7ux/D9ZnY/T6sksfN7KpGPXwCH&#43;b2nYaJg=="></script>



    </body>
</html>
