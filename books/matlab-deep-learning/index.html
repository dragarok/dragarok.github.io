<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="ie=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="author" content="[Alok Regmi]">
<meta name="description" content="### Author: Phil Kim
loc 520 - Keep in mind that the hidden layer becomes ineffective when the hidden nodes have linear activation functions.
loc 660 - The mini batch method , when it selects an appropriate number of data points, obtains the benefits from both methods: speed from the SGD and stability from the batch.
loc 662 - Now, let’s delve a bit into the SGD , batch, and mini batch in terms of the epoch. The epoch is briefly introduced in the “Training of a Single-Layer Neural Network: Delta Rule” section. As a recap, the epoch is the number of completed training cycles for all of the training data. In the batch method, the number of training cycles of the neural network equals an epoch, as shown in Figure 2-18. This makes perfect sense because the batch method utilizes all of the data for one training process.
" />
<meta name="keywords" content="Blog" />
<meta name="robots" content="noodp" />
<meta name="theme-color" content="" />
<link rel="canonical" href="https://dragarok.github.io/books/matlab-deep-learning/" />


    <title>
        
            MATLAB Deep Learning ::  
        
    </title>



<link href="https://cdnjs.cloudflare.com/ajax/libs/flag-icon-css/3.2.1/css/flag-icon.min.css" rel="stylesheet"
    type="text/css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">
    
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js" integrity="sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4" crossorigin="anonymous"></script>
    
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js" integrity="sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa" crossorigin="anonymous"
    onload="renderMathInElement(document.body);"></script>





<link rel="stylesheet" href="../../main.min.daa43c14e17ef1950fa593909b64d3a2d9855d65b354823b6d82fca285818e75.css">




    <link rel="apple-touch-icon" sizes="180x180" href="../../apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="../../favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="../../favicon-16x16.png">
    <link rel="manifest" href="../../site.webmanifest">
    <link rel="mask-icon" href="../../safari-pinned-tab.svg" color="#252627">
    <link rel="shortcut icon" href="../../favicon.ico">
    <meta name="msapplication-TileColor" content="#252627">
    <meta name="theme-color" content="#252627">


  <meta itemprop="name" content="MATLAB Deep Learning">
  <meta itemprop="description" content="### Author: Phil Kim
loc 520 - Keep in mind that the hidden layer becomes ineffective when the hidden nodes have linear activation functions.
loc 660 - The mini batch method , when it selects an appropriate number of data points, obtains the benefits from both methods: speed from the SGD and stability from the batch.
loc 662 - Now, let’s delve a bit into the SGD , batch, and mini batch in terms of the epoch. The epoch is briefly introduced in the “Training of a Single-Layer Neural Network: Delta Rule” section. As a recap, the epoch is the number of completed training cycles for all of the training data. In the batch method, the number of training cycles of the neural network equals an epoch, as shown in Figure 2-18. This makes perfect sense because the batch method utilizes all of the data for one training process.">
  <meta itemprop="datePublished" content="2020-04-26T03:07:44+05:45">
  <meta itemprop="dateModified" content="2020-04-26T03:07:44+05:45">
  <meta itemprop="wordCount" content="149">
  <meta itemprop="image" content="https://dragarok.github.io/">
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:image" content="https://dragarok.github.io/">
  <meta name="twitter:title" content="MATLAB Deep Learning">
  <meta name="twitter:description" content="### Author: Phil Kim
loc 520 - Keep in mind that the hidden layer becomes ineffective when the hidden nodes have linear activation functions.
loc 660 - The mini batch method , when it selects an appropriate number of data points, obtains the benefits from both methods: speed from the SGD and stability from the batch.
loc 662 - Now, let’s delve a bit into the SGD , batch, and mini batch in terms of the epoch. The epoch is briefly introduced in the “Training of a Single-Layer Neural Network: Delta Rule” section. As a recap, the epoch is the number of completed training cycles for all of the training data. In the batch method, the number of training cycles of the neural network equals an epoch, as shown in Figure 2-18. This makes perfect sense because the batch method utilizes all of the data for one training process.">





    <meta property="article:published_time" content="2020-04-26 03:07:44 &#43;0545 &#43;0545" />








    </head>

    <body class="">
        <div class="container">
            <header class="header">
    <span class="header__inner">
        <a href="../../" style="text-decoration: none;">
    <div class="logo">
        
            <span class="logo__mark">~</span>
            <span class="logo__text">K4iv41y4</span>
            <span class="logo__mark">&nbsp;~</span>
            <span class="logo__cursor" style=
                  "
                   
                   ">
            </span>
        
    </div>
</a>


        <span class="header__right">
            
                <nav class="menu">
    <ul class="menu__inner"><li><a href="https://dragarok.github.io/about/">About</a></li><li><a href="https://dragarok.github.io/ai/">AI</a></li><li><a href="https://dragarok.github.io/posts/">Blog</a></li><li><a href="https://dragarok.github.io/books/">Books</a></li><li><a href="https://dragarok.github.io/braindump/">Braindump</a></li>
    </ul>
</nav>

                <span class="menu-trigger">
                    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
                        <path d="M0 0h24v24H0z" fill="none"/>
                        <path d="M3 18h18v-2H3v2zm0-5h18v-2H3v2zm0-7v2h18V6H3z"/>
                    </svg>
                </span>
            

            <span class="theme-toggle unselectable"><svg class="theme-toggler" width="24" height="24" viewBox="0 0 48 48" fill="none" xmlns="http://www.w3.org/2000/svg">
  <path d="M22 41C32.4934 41 41 32.4934 41 22C41 11.5066 32.4934 3 22
  3C11.5066 3 3 11.5066 3 22C3 32.4934 11.5066 41 22 41ZM7 22C7
  13.7157 13.7157 7 22 7V37C13.7157 37 7 30.2843 7 22Z"/>
</svg>
</span>
        </span>
    </span>
</header>


            <div class="content">
                
    <main class="post">

        <div class="post-info">
            
            </p>
        </div>

        <article>
            <h2 class="post-title"><a href="https://dragarok.github.io/books/matlab-deep-learning/">MATLAB Deep Learning</a></h2>

            

            <div class="post-content">
                <p>### Author: Phil Kim</p>
<ul>
<li>
<p>loc 520 - Keep in mind that the hidden layer becomes ineffective when the hidden nodes have linear activation functions.</p>
</li>
<li>
<p>loc 660 - The mini batch method , when it selects an appropriate number of data points, obtains the benefits from both methods: speed from the SGD and stability from the batch.</p>
</li>
<li>
<p>loc 662 - Now, let’s delve a bit into the SGD , batch, and mini batch in terms of the epoch. The epoch is briefly introduced in the “Training of a Single-Layer Neural Network: Delta Rule” section. As a recap, the epoch is the number of completed training cycles for all of the training data. In the batch method, the number of training cycles of the neural network equals an epoch, as shown in Figure 2-18. This makes perfect sense because the batch method utilizes all of the data for one training process.</p>
</li>
</ul>

            </div>
        </article>

        <hr />

        <div class="post-info">
  			</div>

        
    </main>

            </div>

        </div>

        




<script type="text/javascript" src="../../bundle.min.74fe699bb673e8362137b575513abfcf73303855d923eea09c0e507deab0ca7f8321880b672790b9e63cc109a18189deebfd13899a8ff536e858791973ffd487.js" integrity="sha512-dP5pm7Zz6DYhN7V1UTq/z3MwOFXZI&#43;6gnA5Qfeqwyn&#43;DIYgLZyeQueY8wQmhgYne6/0TiZqP9TboWHkZc//Uhw=="></script>



    </body>
</html>
