<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>vision on Alok&#39;s Blog</title>
    <link>https://dragarok.github.io/tags/vision/</link>
    <description>Recent content in vision on Alok&#39;s Blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 19 Apr 2020 19:28:53 +0545</lastBuildDate><atom:link href="https://dragarok.github.io/tags/vision/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Efficient Nets</title>
      <link>https://dragarok.github.io/braindump/2020/04/efficient-nets/</link>
      <pubDate>Sun, 19 Apr 2020 19:28:53 +0545</pubDate>
      
      <guid>https://dragarok.github.io/braindump/2020/04/efficient-nets/</guid>
      <description>Efficient Nets:   Why all this ? &amp;ndash; Scaling CNN’s only in one direction (eg depth only) will result in rapidly deteriorating gains relative to the computational increase needed.
&amp;ndash; ResNet 1000 isn’t much more accurate than ResNet152 for example, as after 100 -150 layer’s gains rapidly drop off.
&amp;ndash; Scaling depth, width and resolution all benifits quickly saturate so not at all possibility of sota.
  We now know the problem so let&amp;rsquo;s do this instead: &amp;ndash; In order to scale up efficiently, all dimensions of depth, width and resolution have to be scaled together, and there is an optimal balance for each dimension relative to the others.</description>
    </item>
    
    <item>
      <title>Convolution</title>
      <link>https://dragarok.github.io/braindump/2020/04/convolution/</link>
      <pubDate>Sun, 19 Apr 2020 16:00:43 +0545</pubDate>
      
      <guid>https://dragarok.github.io/braindump/2020/04/convolution/</guid>
      <description>Convolution Image will be of shape: (H * W * channels)
Our kernel will also be of shape (Hk * Wk * channels)
The number of channels is same as it is volume convolution.
In case of 1 * 1 convolution, the convolution helps decrease or increase the feature map dimensions.</description>
    </item>
    
    <item>
      <title>Image Processing Pipeline</title>
      <link>https://dragarok.github.io/braindump/2020/04/image-processing-pipeline/</link>
      <pubDate>Sun, 19 Apr 2020 16:00:34 +0545</pubDate>
      
      <guid>https://dragarok.github.io/braindump/2020/04/image-processing-pipeline/</guid>
      <description>Integrating image processing with machine learning consists of following steps:
  Decide your project title/purpose/objectives.
  Collect data, i.e images relevant to your project.
  Preprocess all the collected images. Preprocessing steps may be different for different projects. It basically includes filtering, noise removal, grayscale conversion, binary image formation, morphological operation, thresholding and so on.
  Divide the preprocessed images into two sets: training and tesing sets.</description>
    </item>
    
    <item>
      <title>Efficient Nets</title>
      <link>https://dragarok.github.io/ai/efficient_nets/</link>
      <pubDate>Fri, 20 Mar 2020 00:00:00 +0545</pubDate>
      
      <guid>https://dragarok.github.io/ai/efficient_nets/</guid>
      <description>Efficient Nets:   Why all this ? &amp;ndash; Scaling CNN’s only in one direction (eg depth only) will result in rapidly deteriorating gains relative to the computational increase needed.
&amp;ndash; ResNet 1000 isn’t much more accurate than ResNet152 for example, as after 100 -150 layer’s gains rapidly drop off.
&amp;ndash; Scaling depth, width and resolution all benifits quickly saturate so not at all possibility of sota.
  We now know the problem so let&amp;rsquo;s do this instead: &amp;ndash; In order to scale up efficiently, all dimensions of depth, width and resolution have to be scaled together, and there is an optimal balance for each dimension relative to the others.</description>
    </item>
    
    <item>
      <title>Image Processing Pipeline</title>
      <link>https://dragarok.github.io/ai/image_processing_pipeline/</link>
      <pubDate>Fri, 24 Jan 2020 00:00:00 +0545</pubDate>
      
      <guid>https://dragarok.github.io/ai/image_processing_pipeline/</guid>
      <description>Integrating image processing with machine learning consists of following steps:
  Decide your project title/purpose/objectives.
  Collect data, i.e images relevant to your project.
  Preprocess all the collected images. Preprocessing steps may be different for different projects. It basically includes filtering, noise removal, grayscale conversion, binary image formation, morphological operation, thresholding and so on.
  Divide the preprocessed images into two sets: training and tesing sets.</description>
    </item>
    
    <item>
      <title>Convolution</title>
      <link>https://dragarok.github.io/ai/convolution/</link>
      <pubDate>Thu, 05 Dec 2019 00:00:00 +0545</pubDate>
      
      <guid>https://dragarok.github.io/ai/convolution/</guid>
      <description>Convolution Image will be of shape: (H * W * channels)
Our kernel will also be of shape (Hk * Wk * channels)
The number of channels is same as it is volume convolution.
In case of 1 * 1 convolution, the convolution helps decrease or increase the feature map dimensions.</description>
    </item>
    
  </channel>
</rss>
