<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Vision on </title>
    <link>https://dragarok.github.io/tags/vision/</link>
    <description>Recent content in Vision on </description>
    <generator>Hugo</generator>
    <language>en</language>
    <lastBuildDate>Sat, 02 May 2020 04:04:27 +0545</lastBuildDate>
    <atom:link href="https://dragarok.github.io/tags/vision/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Efficient Nets</title>
      <link>https://dragarok.github.io/braindump/2020/04/efficient-nets/</link>
      <pubDate>Sun, 19 Apr 2020 19:28:53 +0545</pubDate>
      <guid>https://dragarok.github.io/braindump/2020/04/efficient-nets/</guid>
      <description>Efficient Nets: Why all this ? &amp;ndash; Scaling CNN’s only in one direction (eg depth only) will result in rapidly deteriorating gains relative to the computational increase needed.&#xA;&amp;ndash; ResNet 1000 isn’t much more accurate than ResNet152 for example, as after 100 -150 layer’s gains rapidly drop off.&#xA;&amp;ndash; Scaling depth, width and resolution all benifits quickly saturate so not at all possibility of sota.&#xA;We now know the problem so let&amp;rsquo;s do this instead: &amp;ndash; In order to scale up efficiently, all dimensions of depth, width and resolution have to be scaled together, and there is an optimal balance for each dimension relative to the others.</description>
    </item>
    <item>
      <title>Convolution</title>
      <link>https://dragarok.github.io/braindump/2020/04/convolution/</link>
      <pubDate>Sun, 19 Apr 2020 16:00:43 +0545</pubDate>
      <guid>https://dragarok.github.io/braindump/2020/04/convolution/</guid>
      <description>Convolution Image will be of shape: (H * W * channels)&#xA;Our kernel will also be of shape (Hk * Wk * channels)&#xA;The number of channels is same as it is volume convolution.&#xA;In case of 1 * 1 convolution, the convolution helps decrease or increase the feature map dimensions.</description>
    </item>
    <item>
      <title>Image Processing Pipeline</title>
      <link>https://dragarok.github.io/braindump/2020/04/image-processing-pipeline/</link>
      <pubDate>Sun, 19 Apr 2020 16:00:34 +0545</pubDate>
      <guid>https://dragarok.github.io/braindump/2020/04/image-processing-pipeline/</guid>
      <description>Integrating image processing with machine learning consists of following steps:&#xA;Decide your project title/purpose/objectives.&#xA;Collect data, i.e images relevant to your project.&#xA;Preprocess all the collected images. Preprocessing steps may be different for different projects. It basically includes filtering, noise removal, grayscale conversion, binary image formation, morphological operation, thresholding and so on.&#xA;Divide the preprocessed images into two sets: training and tesing sets.&#xA;Extract features that you think will most probably define your data.</description>
    </item>
    <item>
      <title>Efficient Nets</title>
      <link>https://dragarok.github.io/ai/efficient_nets/</link>
      <pubDate>Fri, 20 Mar 2020 00:00:00 +0000</pubDate>
      <guid>https://dragarok.github.io/ai/efficient_nets/</guid>
      <description>Efficient Nets: Why all this ? &amp;ndash; Scaling CNN’s only in one direction (eg depth only) will result in rapidly deteriorating gains relative to the computational increase needed.&#xA;&amp;ndash; ResNet 1000 isn’t much more accurate than ResNet152 for example, as after 100 -150 layer’s gains rapidly drop off.&#xA;&amp;ndash; Scaling depth, width and resolution all benifits quickly saturate so not at all possibility of sota.&#xA;We now know the problem so let&amp;rsquo;s do this instead: &amp;ndash; In order to scale up efficiently, all dimensions of depth, width and resolution have to be scaled together, and there is an optimal balance for each dimension relative to the others.</description>
    </item>
    <item>
      <title>Image Processing Pipeline</title>
      <link>https://dragarok.github.io/ai/image_processing_pipeline/</link>
      <pubDate>Fri, 24 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://dragarok.github.io/ai/image_processing_pipeline/</guid>
      <description>Integrating image processing with machine learning consists of following steps:&#xA;Decide your project title/purpose/objectives.&#xA;Collect data, i.e images relevant to your project.&#xA;Preprocess all the collected images. Preprocessing steps may be different for different projects. It basically includes filtering, noise removal, grayscale conversion, binary image formation, morphological operation, thresholding and so on.&#xA;Divide the preprocessed images into two sets: training and tesing sets.&#xA;Extract features that you think will most probably define your data.</description>
    </item>
    <item>
      <title>Convolution</title>
      <link>https://dragarok.github.io/ai/convolution/</link>
      <pubDate>Thu, 05 Dec 2019 00:00:00 +0000</pubDate>
      <guid>https://dragarok.github.io/ai/convolution/</guid>
      <description>Convolution Image will be of shape: (H * W * channels)&#xA;Our kernel will also be of shape (Hk * Wk * channels)&#xA;The number of channels is same as it is volume convolution.&#xA;In case of 1 * 1 convolution, the convolution helps decrease or increase the feature map dimensions.</description>
    </item>
  </channel>
</rss>
